{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13033311,"sourceType":"datasetVersion","datasetId":8252616},{"sourceId":13059347,"sourceType":"datasetVersion","datasetId":8269904},{"sourceId":13059565,"sourceType":"datasetVersion","datasetId":8270032},{"sourceId":13077907,"sourceType":"datasetVersion","datasetId":8282787},{"sourceId":13179259,"sourceType":"datasetVersion","datasetId":8351686}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTANT TOKEN SETTINGS AND INSTALLS","metadata":{}},{"cell_type":"code","source":"import os; os.environ[\"HF_TOKEN\"] =\"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\"   \nhf_token = os.environ.get(\"HF_TOKEN\")\nos.environ[\"SERPAPI_API_KEY\"] =\"56604132ea94bbed4735add33bb2b3a57a61e9b68f9b2074e43114a1eebe663f\" \nserpapi=os.environ.get(\"SERPAPI_API_KEY\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:04.039943Z","iopub.execute_input":"2025-09-14T21:43:04.040231Z","iopub.status.idle":"2025-09-14T21:43:04.046533Z","shell.execute_reply.started":"2025-09-14T21:43:04.040211Z","shell.execute_reply":"2025-09-14T21:43:04.046012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Use the secret you created\nimport os\nhf_token = os.environ.get(\"HF_TOKEN\")\n\n# Authenticate\nif hf_token:\n    api = HfApi()\n    api.whoami(token=hf_token)\n    print(\"Successfully logged into Hugging Face.\")\nelse:\n    print(\"Hugging Face token not found. Please add it as a Kaggle Secret with the label 'HF_TOKEN'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:04.358467Z","iopub.execute_input":"2025-09-14T21:43:04.358902Z","iopub.status.idle":"2025-09-14T21:43:04.927622Z","shell.execute_reply.started":"2025-09-14T21:43:04.358859Z","shell.execute_reply":"2025-09-14T21:43:04.926912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install serpapi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:04.928704Z","iopub.execute_input":"2025-09-14T21:43:04.929184Z","iopub.status.idle":"2025-09-14T21:43:10.769464Z","shell.execute_reply.started":"2025-09-14T21:43:04.929165Z","shell.execute_reply":"2025-09-14T21:43:10.768670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Imports**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport logging\nimport requests\nimport json\nimport ast\nimport re\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Dict, Tuple, Optional, Any, Union\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport pickle\nfrom pathlib import Path\n\n# Core ML libraries\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sentence_transformers import SentenceTransformer, util\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, roc_auc_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# External API libraries\nimport serpapi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:10.770772Z","iopub.execute_input":"2025-09-14T21:43:10.771036Z","iopub.status.idle":"2025-09-14T21:43:51.865393Z","shell.execute_reply.started":"2025-09-14T21:43:10.771011Z","shell.execute_reply":"2025-09-14T21:43:51.864795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration and logging","metadata":{}},{"cell_type":"code","source":"logging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='urllib3')\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.866174Z","iopub.execute_input":"2025-09-14T21:43:51.866727Z","iopub.status.idle":"2025-09-14T21:43:51.870728Z","shell.execute_reply.started":"2025-09-14T21:43:51.866698Z","shell.execute_reply":"2025-09-14T21:43:51.870021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DATA STRUCTURES AND ENUMS","metadata":{}},{"cell_type":"code","source":"class ProcessingMode(Enum):\n    DETECTION_ONLY = \"detection_only\"\n    MITIGATION_GUIDED = \"mitigation_guided\"\n    HYBRID_RESEARCH = \"hybrid_research\"\n\nclass RoutingStrategy(Enum):\n    FASTEST = \"fastest\"\n    MOST_ACCURATE = \"most_accurate\"\n    COST_EFFECTIVE = \"cost_effective\"\n    DOMAIN_OPTIMIZED = \"domain_optimized\"\n\n@dataclass\nclass QueryFeatures:\n    \"\"\"Features extracted from a query for routing decisions\"\"\"\n    query_length: int\n    entity_count: int\n    temporal_references: int\n    domain_category: str\n    complexity_score: float\n    factual_density: float\n    confidence_level: float\n\n@dataclass\nclass DetectionResult:\n    \"\"\"Result from a hallucination detection method\"\"\"\n    method_name: str\n    is_hallucination: bool\n    confidence: float\n    execution_time: float\n    metadata: Dict[str, Any]\n\n@dataclass\nclass EnsembleResult:\n    \"\"\"Final result from ensemble processing\"\"\"\n    final_prediction: bool\n    confidence: float\n    method_votes: Dict[str, bool]\n    method_confidences: Dict[str, float]\n    consensus_level: float\n    processing_path: str\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.872939Z","iopub.execute_input":"2025-09-14T21:43:51.873243Z","iopub.status.idle":"2025-09-14T21:43:51.902391Z","shell.execute_reply.started":"2025-09-14T21:43:51.873225Z","shell.execute_reply":"2025-09-14T21:43:51.901839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CORE DETECTION FUNCTIONS","metadata":{}},{"cell_type":"code","source":"class DetectionMethods:\n    \"\"\"\n    Container for your friend's core detection implementations\n    Enhanced with timing and confidence scoring\n    \"\"\"\n    \n    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n                 embedding_model: SentenceTransformer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.embedding_model = embedding_model\n        \n        # Initialize API keys\n        try:\n            # In a non-Kaggle environment, you would use os.environ.get(\"SERPAPI_API_KEY\")\n            self.serp_api_key = \"56604132ea94bbed4735add33bb2b3a57a61e9b68f9b2074e43114a1eebe663f\" #UserSecretsClient().get_secret(\"SERPAPI_API_KEY\")\n        except:\n            self.serp_api_key = None\n            logger.warning(\"SERPAPI_API_KEY not found. Web search will be disabled.\")\n\n    def call_llm_batch(self, prompts: List[List[Dict[str, str]]], \n                      model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> List[str]:\n        \"\"\"Friend's batch inference implementation\"\"\"\n        if not model or not tokenizer:\n            logger.error(\"Model or tokenizer not loaded\")\n            return [\"\"] * len(prompts)\n\n        # Apply chat template\n        formatted_prompts = [\n            tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)\n            for p in prompts\n        ]\n\n        # Tokenize\n        inputs = tokenizer(\n            formatted_prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=1024\n        ).to(model.device)\n\n        # Generate\n        terminators = [\n            tokenizer.eos_token_id,\n            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n        ]\n\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=256,\n            eos_token_id=terminators,\n            do_sample=False\n        )\n\n        # Decode\n        response_texts = tokenizer.batch_decode(\n            outputs[:, inputs['input_ids'].shape[-1]:],\n            skip_special_tokens=True\n        )\n\n        return [text.strip() for text in response_texts]\n\n    def extract_python_list_from_string(self, s: str) -> list:\n        \"\"\"Friend's list parsing utility\"\"\"\n        match = re.search(r'\\[.*\\]', s, re.DOTALL)\n        if match:\n            list_str = match.group(0)\n            try:\n                return ast.literal_eval(list_str)\n            except (ValueError, SyntaxError):\n                return None\n        return None\n\n    def get_wikidata_entities(self, entity_name: str, limit: int = 5) -> list:\n        \"\"\"Friend's Wikidata helper\"\"\"\n        url = \"https://www.wikidata.org/w/api.php\"\n        params = {\n            \"action\": \"wbsearchentities\",\n            \"format\": \"json\", \n            \"language\": \"en\",\n            \"search\": entity_name,\n            \"limit\": limit\n        }\n        try:\n            response = requests.get(url, params=params, \n                                  headers={'User-Agent': 'HELIOS/1.0'}, \n                                  timeout=5)\n            response.raise_for_status()\n            return response.json().get(\"search\", [])\n        except requests.RequestException:\n            return []\n\n    def get_wikidata_triples(self, entity_id: str) -> list:\n        \"\"\"Friend's Wikidata SPARQL query\"\"\"\n        endpoint_url = \"https://query.wikidata.org/sparql\"\n        query = f\"\"\"\n        SELECT ?propLabel ?valueLabel WHERE {{\n            wd:{entity_id} ?p ?statement .\n            ?property wikibase:directClaim ?p .\n            ?property rdfs:label ?propLabel .\n            ?statement ?ps ?value .\n            ?value rdfs:label ?valueLabel .\n            FILTER(LANG(?propLabel) = \"en\")\n            FILTER(LANG(?valueLabel) = \"en\")\n        }} LIMIT 25\n        \"\"\"\n        try:\n            response = requests.get(endpoint_url, \n                                  params={'query': query, 'format': 'json'},\n                                  headers={'User-Agent': 'HELIOS/1.0'}, \n                                  timeout=10)\n            response.raise_for_status()\n            data = response.json()\n            triples = [(item.get(\"propLabel\", {}).get(\"value\"), \n                       item.get(\"valueLabel\", {}).get(\"value\"))\n                      for item in data.get(\"results\", {}).get(\"bindings\", [])]\n            return list(set(triples))\n        except requests.RequestException:\n            return []\n\n    def perform_web_search(self, query: str) -> str:\n        \"\"\"Friend's web search implementation\"\"\"\n        if not self.serp_api_key:\n            return \"No web search available.\"\n        \n        try:\n            client = serpapi.Client(api_key=self.serp_api_key)\n            params = {'engine': 'google', 'q': query}\n            results_dict = client.search(**params)\n            \n            snippets = []\n            if \"answer_box\" in results_dict and \"snippet\" in results_dict[\"answer_box\"]:\n                snippets.append(results_dict[\"answer_box\"][\"snippet\"])\n            if \"organic_results\" in results_dict:\n                for result in results_dict[\"organic_results\"][:3]:\n                    if \"snippet\" in result:\n                        snippets.append(result[\"snippet\"])\n            \n            return \" \".join(snippets) if snippets else \"No relevant information found.\"\n        except Exception as e:\n            return f\"Search error: {e}\"\n\n    def kgr_detect_hallucination(self, answer: str) -> DetectionResult:\n        \"\"\"Friend's KGR implementation with enhanced result structure\"\"\"\n        start_time = time.time()\n        \n        # Extract claims\n        claim_extraction_messages = [[\n            {\"role\": \"system\", \"content\": \"Extract factual claims from the text. Return only a Python list.\"},\n            {\"role\": \"user\", \"content\": f\"Text: \\\"{answer}\\\"\\nExtracted Claims:\"}\n        ]]\n        \n        claims_response = self.call_llm_batch(claim_extraction_messages, self.model, self.tokenizer)[0]\n        claims = self.extract_python_list_from_string(claims_response)\n        \n        if not claims or not isinstance(claims, list):\n            return DetectionResult(\n                method_name=\"KGR\",\n                is_hallucination=False,\n                confidence=0.0,\n                execution_time=time.time() - start_time,\n                metadata={\"error\": \"Could not extract claims\", \"claims\": []}\n            )\n\n        # Process each claim\n        verified_claims = 0\n        total_claims = len(claims)\n        hallucination_detected = False\n\n        for claim in claims:\n            # Extract entities\n            entity_messages = [[\n                {\"role\": \"system\", \"content\": \"Extract key entities. Return only a Python list.\"},\n                {\"role\": \"user\", \"content\": f\"Claim: \\\"{claim}\\\"\\nEntities:\"}\n            ]]\n            entity_response = self.call_llm_batch(entity_messages, self.model, self.tokenizer)[0]\n            entities = self.extract_python_list_from_string(entity_response)\n            \n            if not entities:\n                continue\n\n            # Get knowledge graph data\n            all_triples = []\n            for entity_name in entities:\n                search_results = self.get_wikidata_entities(entity_name, limit=1)\n                if search_results:\n                    triples = self.get_wikidata_triples(search_results[0]['id'])\n                    all_triples.extend([(entity_name, prop, val) for prop, val in triples if prop and val])\n\n            # Verify claim against knowledge graph\n            if all_triples:\n                relevant_facts = \"\\n\".join([f\"- {' '.join(fact)}\" for fact in all_triples])\n                verification_messages = [[\n                    {\"role\": \"system\", \"content\": \"Based only on the evidence, is the claim True or False? Answer with one word.\"},\n                    {\"role\": \"user\", \"content\": f\"Evidence:\\n{relevant_facts}\\n\\nClaim: \\\"{claim}\\\"\\nVerdict:\"}\n                ]]\n                verdict = self.call_llm_batch(verification_messages, self.model, self.tokenizer)[0]\n                \n                if 'false' in verdict.lower():\n                    hallucination_detected = True\n                    break\n                verified_claims += 1\n\n        confidence = verified_claims / total_claims if total_claims > 0 else 0.0\n        \n        return DetectionResult(\n            method_name=\"KGR\",\n            is_hallucination=hallucination_detected,\n            confidence=1.0 - confidence if hallucination_detected else confidence,\n            execution_time=time.time() - start_time,\n            metadata={\"claims_processed\": total_claims, \"verified_claims\": verified_claims, \"claims\": claims}\n        )\n\n    def stitch_detect_hallucination(self, answer: str) -> DetectionResult:\n        \"\"\"Friend's Stitch in Time implementation\"\"\"\n        start_time = time.time()\n        \n        # Extract concepts\n        concept_extraction_messages = [[\n            {\"role\": \"system\", \"content\": \"Extract key verifiable concepts. Return comma-separated list.\"},\n            {\"role\": \"user\", \"content\": f\"Sentence: \\\"{answer}\\\"\"}\n        ]]\n        concepts_str = self.call_llm_batch(concept_extraction_messages, self.model, self.tokenizer)[0]\n        \n        if not concepts_str:\n            return DetectionResult(\n                method_name=\"Stitch\",\n                is_hallucination=False,\n                confidence=0.0,\n                execution_time=time.time() - start_time,\n                metadata={\"error\": \"No concepts extracted\"}\n            )\n\n        concepts = [concept.strip() for concept in concepts_str.split(',')]\n        verified_concepts = 0\n        total_concepts = len(concepts)\n\n        for concept in concepts:\n            if not concept:\n                continue\n\n            # Generate validation question\n            question_gen_messages = [[\n                {\"role\": \"system\", \"content\": \"Generate a yes/no question to test the concept. Return only the question.\"},\n                {\"role\": \"user\", \"content\": f\"Sentence: \\\"{answer}\\\"\\nConcept: \\\"{concept}\\\"\"}\n            ]]\n            validation_question = self.call_llm_batch(question_gen_messages, self.model, self.tokenizer)[0]\n\n            if not validation_question:\n                continue\n\n            # Search for evidence\n            context = self.perform_web_search(validation_question)\n\n            # Verify against context\n            verdict_messages = [[\n                {\"role\": \"system\", \"content\": \"Based only on search results, answer Yes or No. If unsure, answer No.\"},\n                {\"role\": \"user\", \"content\": f\"Search Results: \\\"{context}\\\"\\nQuestion: \\\"{validation_question}\\\"\"}\n            ]]\n            verdict_answer = self.call_llm_batch(verdict_messages, self.model, self.tokenizer)[0].lower()\n\n            if \"yes\" in verdict_answer and \"no\" not in verdict_answer:\n                verified_concepts += 1\n            else:\n                # First failure indicates hallucination\n                return DetectionResult(\n                    method_name=\"Stitch\",\n                    is_hallucination=True,\n                    confidence=0.8,  # High confidence in detection\n                    execution_time=time.time() - start_time,\n                    metadata={\"failed_concept\": concept, \"concepts_checked\": len([c for c in concepts[:concepts.index(concept)+1] if c])}\n                )\n\n        confidence = verified_concepts / total_concepts if total_concepts > 0 else 0.0\n        \n        return DetectionResult(\n            method_name=\"Stitch\",\n            is_hallucination=False,\n            confidence=confidence,\n            execution_time=time.time() - start_time,\n            metadata={\"concepts_verified\": verified_concepts, \"total_concepts\": total_concepts}\n        )\n\n    def interrogate_llm_detect_hallucination(self, question: str, answer: str, \n                                           k_iterations: int = 5, \n                                           similarity_threshold: float = 0.91) -> DetectionResult:\n        \"\"\"Friend's InterrogateLLM implementation\"\"\"\n        start_time = time.time()\n\n        if not self.embedding_model:\n            return DetectionResult(\n                method_name=\"InterrogateLLM\",\n                is_hallucination=False,\n                confidence=0.0,\n                execution_time=time.time() - start_time,\n                metadata={\"error\": \"Embedding model not available\"}\n            )\n\n        # Generate reconstructed questions\n        reconstruction_messages = []\n        for _ in range(k_iterations):\n            reconstruction_messages.append([\n                {\"role\": \"system\", \"content\": \"Given this answer, what was the most likely question? Return only the question.\"},\n                {\"role\": \"user\", \"content\": f\"Answer: \\\"{answer}\\\"\\nQuestion:\"}\n            ])\n\n        reconstructed_questions = self.call_llm_batch(reconstruction_messages, self.model, self.tokenizer)\n\n        # Calculate similarities\n        original_embedding = self.embedding_model.encode(question, convert_to_tensor=True)\n        reconstructed_embeddings = self.embedding_model.encode(reconstructed_questions, convert_to_tensor=True)\n        \n        cosine_scores = util.cos_sim(original_embedding, reconstructed_embeddings)\n        similarity_scores = cosine_scores.flatten().tolist()\n        average_similarity = np.mean(similarity_scores)\n\n        is_hallucination = average_similarity < similarity_threshold\n        confidence = abs(average_similarity - similarity_threshold) / similarity_threshold\n\n        return DetectionResult(\n            method_name=\"InterrogateLLM\",\n            is_hallucination=is_hallucination,\n            confidence=confidence,\n            execution_time=time.time() - start_time,\n            metadata={\n                \"average_similarity\": average_similarity,\n                \"similarity_scores\": similarity_scores,\n                \"threshold\": similarity_threshold,\n                \"reconstructed_questions\": reconstructed_questions\n            }\n        )\n\n    def cove_detect_hallucination(self, question: str, answer: str) -> DetectionResult:\n        \"\"\"Friend's Chain-of-Verification implementation\"\"\"\n        start_time = time.time()\n\n        # Plan verification questions\n        plan_messages = [[\n            {\"role\": \"system\", \"content\": \"Generate verification questions for fact-checking. Return only a Python list.\"},\n            {\"role\": \"user\", \"content\": f\"Query: \\\"{question}\\\"\\nAnswer: \\\"{answer}\\\"\"}\n        ]]\n        questions_response = self.call_llm_batch(plan_messages, self.model, self.tokenizer)[0]\n        verification_questions = self.extract_python_list_from_string(questions_response)\n\n        if not verification_questions or not isinstance(verification_questions, list):\n            return DetectionResult(\n                method_name=\"CoVE\",\n                is_hallucination=False,\n                confidence=0.0,\n                execution_time=time.time() - start_time,\n                metadata={\"error\": \"Could not generate verification questions\"}\n            )\n\n        # Execute verifications\n        execution_messages = []\n        for v_question in verification_questions:\n            execution_messages.append([\n                {\"role\": \"system\", \"content\": \"Answer this question factually and concisely.\"},\n                {\"role\": \"user\", \"content\": v_question}\n            ])\n\n        verification_answers = self.call_llm_batch(execution_messages, self.model, self.tokenizer)\n\n        # Check for contradictions\n        contradictions = 0\n        for i, (v_question, v_answer) in enumerate(zip(verification_questions, verification_answers)):\n            contradiction_messages = [[\n                {\"role\": \"system\", \"content\": \"Does the verified fact contradict the original answer? Answer Yes or No.\"},\n                {\"role\": \"user\", \"content\": f\"Original: \\\"{answer}\\\"\\nVerified: '{v_question}' -> '{v_answer}'\\nContradiction?\"}\n            ]]\n            verdict = self.call_llm_batch(contradiction_messages, self.model, self.tokenizer)[0]\n            \n            if 'yes' in verdict.strip().lower():\n                contradictions += 1\n\n        is_hallucination = contradictions > 0\n        confidence = contradictions / len(verification_questions) if verification_questions else 0.0\n\n        return DetectionResult(\n            method_name=\"CoVE\",\n            is_hallucination=is_hallucination,\n            confidence=confidence if is_hallucination else 1.0 - confidence,\n            execution_time=time.time() - start_time,\n            metadata={\n                \"verification_questions\": verification_questions,\n                \"contradictions_found\": contradictions,\n                \"total_verifications\": len(verification_questions)\n            }\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.903099Z","iopub.execute_input":"2025-09-14T21:43:51.903285Z","iopub.status.idle":"2025-09-14T21:43:51.938939Z","shell.execute_reply.started":"2025-09-14T21:43:51.903271Z","shell.execute_reply":"2025-09-14T21:43:51.938394Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ARCHITECTURAL SETTINGS","metadata":{}},{"cell_type":"markdown","source":"## Advanced query analysis for intelligent routing","metadata":{}},{"cell_type":"code","source":"class QueryAnalyzer:\n    \"\"\"\n     Advanced query analysis for intelligent routing (Final, Corrected Version)\n    \"\"\"\n    \n    def __init__(self):\n        # We now separate analytical keywords from simple complexity indicators\n        self.domain_keywords = {\n            'factual': ['what', 'when', 'where', 'who', 'how many', 'date', 'year', 'is it true'],\n            'creative': ['write', 'create', 'imagine', 'story', 'poem', 'creative'],\n            # NEW: A specific list for truly analytical queries\n            'analytical': ['analyze', 'compare', 'evaluate', 'explain', 'why', 'impact', 'because', 'contrast']\n        }\n    \n    def extract_features(self, query: str, answer: str = \"\") -> QueryFeatures:\n        \"\"\"Extract comprehensive features for routing decisions with better text cleaning.\"\"\"\n        \n        cleaned_query = re.sub(r'[^\\w\\s]', '', query.lower())\n        tokens = cleaned_query.split()\n        \n        query_length = len(tokens)\n        entity_count = len(re.findall(r'\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b', query))\n        \n        temporal_patterns = [r'\\b\\d{4}\\b', r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', r'\\b(?:yesterday|today|tomorrow|last|next)\\b']\n        temporal_references = sum(len(re.findall(pattern, query, re.IGNORECASE)) for pattern in temporal_patterns)\n        \n        # --- MODIFIED: Domain classification now uses the new 'analytical' list ---\n        domain_scores = {}\n        for domain, keywords in self.domain_keywords.items():\n            score = 0\n            for keyword in keywords:\n                if ' ' in keyword and keyword in cleaned_query:\n                    score += 1\n                elif keyword in tokens:\n                    score += 1\n            domain_scores[domain] = score\n        \n        # We now prioritize 'analytical' if it appears.\n        if domain_scores.get('analytical', 0) > 0:\n            domain_category = 'analytical'\n        else:\n            domain_category = max(domain_scores, key=lambda k: domain_scores.get(k, 0)) if any(domain_scores.values()) else 'general'\n\n        # --- MODIFIED: Complexity is now just about conjunctions/length, not analysis ---\n        complexity_indicators = ['and', 'but', 'or']\n        # A simple measure: is it a long sentence OR does it contain a conjunction?\n        complexity_score = 1.0 if query_length > 15 or any(ind in tokens for ind in complexity_indicators) else 0.0\n\n        factual_indicators = ['is', 'was', 'are', 'were', 'has', 'have', 'did', 'does']\n        factual_density = min(1.0, sum(1 for indicator in factual_indicators if indicator in tokens) / len(tokens)) if tokens else 0.0\n        \n        confidence_level = 0.8\n        \n        return QueryFeatures(\n            query_length=query_length,\n            entity_count=entity_count,\n            temporal_references=temporal_references,\n            domain_category=domain_category,\n            complexity_score=complexity_score,\n            factual_density=factual_density,\n            confidence_level=confidence_level\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.939582Z","iopub.execute_input":"2025-09-14T21:43:51.939758Z","iopub.status.idle":"2025-09-14T21:43:51.960557Z","shell.execute_reply.started":"2025-09-14T21:43:51.939742Z","shell.execute_reply":"2025-09-14T21:43:51.960088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Intelligent routing system with ML-based decisions","metadata":{}},{"cell_type":"code","source":"class AdaptiveRouter:\n    \"\"\"\n     Intelligent routing system with ML-based decisions (Final, Production-Ready Logic)\n    \"\"\"\n    \n    def __init__(self, model_path: Optional[str] = None):\n        self.routing_model = None\n        self.performance_history = {}\n        self.model_path = model_path\n        self.query_analyzer = QueryAnalyzer() # Assumes you are using the improved QueryAnalyzer from the previous step\n        \n        if model_path and Path(model_path).exists():\n            self.load_model()\n        else:\n            self.routing_model = DecisionTreeClassifier(random_state=42)\n    \n    # ... (load_model, save_model are unchanged) ...\n    def load_model(self):\n        \"\"\"Load pre-trained routing model\"\"\"\n        try:\n            with open(self.model_path, 'rb') as f:\n                self.routing_model = pickle.load(f)\n            logger.info(\"Routing model loaded successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to load routing model: {e}\")\n            self.routing_model = DecisionTreeClassifier(random_state=42)\n    \n    def save_model(self, path: str):\n        \"\"\"Save routing model\"\"\"\n        try:\n            with open(path, 'wb') as f:\n                pickle.dump(self.routing_model, f)\n            logger.info(f\"Routing model saved to {path}\")\n        except Exception as e:\n            logger.error(f\"Failed to save routing model: {e}\")\n\n    # =================== THIS IS THE FINAL, DEFINITIVE VERSION ===================\n    def decide_processing_mode(self, query: str, answer: str = \"\") -> ProcessingMode:\n        \"\"\"\n        Decide which processing path to take using robust, multi-factor rules.\n        \"\"\"\n        features = self.query_analyzer.extract_features(query, answer)\n        \n        # --- Priority 1: Check for explicitly ANALYTICAL queries ---\n        # This remains the same. It's for the most complex tasks.\n        if features.domain_category == 'analytical':\n            logger.info(f\"Routing to HYBRID_RESEARCH because the query is analytical.\")\n            return ProcessingMode.HYBRID_RESEARCH\n\n        # --- Priority 2: Check for FACTUAL queries with robust signals ---\n        # We now look for multiple signals of a fact-checking task.\n        is_short_direct_question = features.query_length < 10 and '?' in query\n        \n        # NEW, MORE ROBUST CHECK: Is the statement rich with entities?\n        # A high number of named entities (people, places, dates) is a very strong\n        # signal that the text is stating facts. We'll use a threshold of 3 or more.\n        is_entity_rich = features.entity_count + features.temporal_references >= 3\n\n        if features.domain_category == 'factual' and (is_short_direct_question or is_entity_rich):\n            logger.info(f\"Routing to DETECTION_ONLY. Reason: Factual domain with either high entity count ({is_entity_rich}) or being a short, direct question ({is_short_direct_question}).\")\n            return ProcessingMode.DETECTION_ONLY\n\n        # --- Priority 3: Fallback for all OTHER queries ---\n        logger.info(f\"Routing to MITIGATION_GUIDED as the default path for domain: {features.domain_category}.\")\n        return ProcessingMode.MITIGATION_GUIDED\n    # =====================================================================\n\n    # ... (select_detection_methods and update_performance are unchanged) ...\n    def select_detection_methods(self, query: str, answer: str) -> List[str]:\n        \"\"\"Select optimal detection methods based on query characteristics\"\"\"\n        features = self.query_analyzer.extract_features(query, answer)\n        \n        # Method selection logic\n        methods = []\n        \n        # Always include baseline methods\n        if query and answer:  # Q+A methods\n            methods.extend([\"CoVE\", \"InterrogateLLM\"])\n        \n        if answer:  # A-only methods\n            methods.extend([\"KGR\", \"Stitch\"])\n        \n        # Priority-based selection based on query characteristics\n        if features.entity_count > 2:\n            methods = [\"KGR\"] + [m for m in methods if m != \"KGR\"]\n        \n        if features.temporal_references > 0:\n            methods = [\"Stitch\"] + [m for m in methods if m != \"Stitch\"]\n        \n        if features.complexity_score > 0.7:\n            methods = [\"CoVE\"] + [m for m in methods if m != \"CoVE\"]\n        \n        return list(dict.fromkeys(methods)) # Return unique methods while preserving order\n    \n    def update_performance(self, query: str, answer: str, method: str, \n                          result: DetectionResult, ground_truth: Optional[bool] = None):\n        \"\"\"Update performance tracking for adaptive learning\"\"\"\n        features = self.query_analyzer.extract_features(query, answer)\n        \n        key = f\"{features.domain_category}_{method}\"\n        if key not in self.performance_history:\n            self.performance_history[key] = {\n                'accuracy_scores': [],\n                'latency_scores': [],\n                'confidence_scores': []\n            }\n        \n        # Track performance metrics\n        if ground_truth is not None:\n            accuracy = 1.0 if result.is_hallucination == ground_truth else 0.0\n            self.performance_history[key]['accuracy_scores'].append(accuracy)\n        \n        self.performance_history[key]['latency_scores'].append(result.execution_time)\n        self.performance_history[key]['confidence_scores'].append(result.confidence)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.961250Z","iopub.execute_input":"2025-09-14T21:43:51.961442Z","iopub.status.idle":"2025-09-14T21:43:51.984335Z","shell.execute_reply.started":"2025-09-14T21:43:51.961427Z","shell.execute_reply":"2025-09-14T21:43:51.983802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Advanced ensemble fusion algorithms","metadata":{}},{"cell_type":"code","source":"class EnsembleFusion:\n    \"\"\"\n     Advanced ensemble fusion algorithms\n    \"\"\"\n    \n    def __init__(self):\n        self.method_weights = {\n            'KGR': 1.0,\n            'CoVE': 1.0, \n            'Stitch': 1.0,\n            'InterrogateLLM': 1.0\n        }\n        self.domain_weights = {}\n    \n    def weighted_voting(self, results: List[DetectionResult]) -> EnsembleResult:\n        \"\"\"Weighted voting based on historical performance\"\"\"\n        if not results:\n            return EnsembleResult(\n                final_prediction=False,\n                confidence=0.0,\n                method_votes={},\n                method_confidences={},\n                consensus_level=0.0,\n                processing_path=\"error\"\n            )\n        \n        votes = {}\n        confidences = {}\n        weighted_sum = 0.0\n        total_weight = 0.0\n        \n        for result in results:\n            method = result.method_name\n            weight = self.method_weights.get(method, 1.0)\n            \n            votes[method] = result.is_hallucination\n            confidences[method] = result.confidence\n            \n            # Weight the vote by method performance and confidence\n            vote_value = 1.0 if result.is_hallucination else 0.0\n            weighted_sum += weight * vote_value * result.confidence\n            total_weight += weight * result.confidence\n        \n        # Final prediction\n        final_score = weighted_sum / total_weight if total_weight > 0 else 0.0\n        final_prediction = final_score > 0.5\n        \n        # Calculate consensus level\n        num_results = len(results)\n        if num_results > 0:\n            positive_votes = sum(1 for v in votes.values() if v)\n            consensus_level = abs(positive_votes - num_results / 2) / (num_results / 2) if num_results > 1 else 1.0\n        else:\n            consensus_level = 0.0\n        \n        return EnsembleResult(\n            final_prediction=final_prediction,\n            confidence=final_score if final_prediction else 1.0 - final_score,\n            method_votes=votes,\n            method_confidences=confidences,\n            consensus_level=consensus_level,\n            processing_path=\"weighted_voting\"\n        )\n    \n    def specialization_routing(self, query: str, results: List[DetectionResult]) -> EnsembleResult:\n        \"\"\"Route based on method specialization\"\"\"\n        # Separate Q+A methods from A-only methods\n        qa_methods = [r for r in results if r.method_name in ['CoVE', 'InterrogateLLM']]\n        \n        # If Q+A methods agree, trust them\n        if len(qa_methods) >= 2:\n            qa_votes = [r.is_hallucination for r in qa_methods]\n            if len(set(qa_votes)) == 1:  # All agree\n                avg_confidence = np.mean([r.confidence for r in qa_methods])\n                return EnsembleResult(\n                    final_prediction=qa_votes[0],\n                    confidence=avg_confidence,\n                    method_votes={r.method_name: r.is_hallucination for r in results},\n                    method_confidences={r.method_name: r.confidence for r in results},\n                    consensus_level=1.0,\n                    processing_path=\"qa_consensus\"\n                )\n        \n        # Fall back to weighted voting\n        return self.weighted_voting(results)\n    \n    def confidence_fusion(self, results: List[DetectionResult], \n                         confidence_threshold: float = 0.8) -> EnsembleResult:\n        \"\"\"Confidence-aware fusion that can abstain from low-confidence decisions\"\"\"\n        \n        # Filter high-confidence results\n        high_confidence_results = [r for r in results if r.confidence >= confidence_threshold]\n        \n        if not high_confidence_results:\n            # No high-confidence results, return uncertain by falling back to standard voting\n            return self.weighted_voting(results)\n        \n        # Use only high-confidence results for decision\n        return self.weighted_voting(high_confidence_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:51.985025Z","iopub.execute_input":"2025-09-14T21:43:51.985235Z","iopub.status.idle":"2025-09-14T21:43:52.007805Z","shell.execute_reply.started":"2025-09-14T21:43:51.985221Z","shell.execute_reply":"2025-09-14T21:43:52.007321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main orchestration system that brings everything together","metadata":{}},{"cell_type":"code","source":"class HeliosCore:\n    \"\"\"\n     Main orchestration system that brings everything together\n    \"\"\"\n    \n    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer,\n                 embedding_model: SentenceTransformer):\n        \n        # Core components\n        self.model = model\n        self.tokenizer = tokenizer\n        self.embedding_model = embedding_model\n        \n        # Your architectural components\n        self.router = AdaptiveRouter()\n        self.fusion_engine = EnsembleFusion()\n        self.query_analyzer = QueryAnalyzer()\n        \n        # Friend's detection methods\n        self.detection_methods = DetectionMethods(model, tokenizer, embedding_model)\n        \n        # Performance tracking\n        self.execution_logs = []\n        self.performance_metrics = {}\n    \n    def process_query(self, query: str, answer: str, \n                     mode: Optional[ProcessingMode] = None) -> Dict[str, Any]:\n        \"\"\"\n        Main processing pipeline that orchestrates the entire HELIOS system\n        \"\"\"\n        start_time = time.time()\n        \n        # Stage 0: Adaptive Routing\n        if mode is None:\n            mode = self.router.decide_processing_mode(query, answer)\n        print(mode)\n        selected_methods = self.router.select_detection_methods(query, answer)\n        \n        logger.info(f\"Processing query with mode: {mode.value}\")\n        logger.info(f\"Selected methods: {selected_methods}\")\n        \n        # Stage 1: Parallel Detection Pipeline\n        detection_results = []\n        \n        for method_name in selected_methods:\n            try:\n                if method_name == \"KGR\":\n                    result = self.detection_methods.kgr_detect_hallucination(answer)\n                elif method_name == \"CoVE\":\n                    result = self.detection_methods.cove_detect_hallucination(query, answer)\n                elif method_name == \"Stitch\":\n                    result = self.detection_methods.stitch_detect_hallucination(answer)\n                elif method_name == \"InterrogateLLM\":\n                    result = self.detection_methods.interrogate_llm_detect_hallucination(query, answer)\n                else:\n                    continue\n                \n                detection_results.append(result)\n                logger.info(f\"{method_name}: {result.is_hallucination} (conf: {result.confidence:.3f})\")\n                \n            except Exception as e:\n                logger.error(f\"Error in {method_name}: {e}\")\n                continue\n        \n        # Stage 2: Intelligent Ensemble Fusion\n        if mode == ProcessingMode.DETECTION_ONLY:\n            ensemble_result = self.fusion_engine.weighted_voting(detection_results)\n        elif mode == ProcessingMode.HYBRID_RESEARCH:\n            ensemble_result = self.fusion_engine.specialization_routing(query, detection_results)\n        else:  # MITIGATION_GUIDED\n            ensemble_result = self.fusion_engine.confidence_fusion(detection_results)\n        \n        # Compile comprehensive results\n        total_time = time.time() - start_time\n        \n        result = {\n            'query': query,\n            'answer': answer,\n            'processing_mode': mode.value,\n            'selected_methods': selected_methods,\n            'individual_results': [asdict(r) for r in detection_results],\n            'ensemble_result': asdict(ensemble_result),\n            'execution_time': total_time,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        # Log for continuous learning\n        self.execution_logs.append(result)\n        \n        return result\n    \n    def batch_process(self, data: List[Dict[str, str]], \n                     ground_truth_col: Optional[str] = None) -> pd.DataFrame:\n        \"\"\"\n        Process multiple queries in batch for benchmarking\n        \"\"\"\n        results = []\n        \n        for item in tqdm(data, desc=\"Processing batch\"):\n            query = item.get('question', '')\n            answer = item.get('answer', '')\n            ground_truth = item.get(ground_truth_col) if ground_truth_col else None\n            \n            result = self.process_query(query, answer)\n            \n            # Add ground truth for evaluation\n            if ground_truth is not None:\n                result['ground_truth'] = ground_truth\n            \n            results.append(result)\n        \n        return pd.DataFrame(results)\n    \n    def evaluate_performance(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"\n         Comprehensive evaluation framework\n        \"\"\"\n        if 'ground_truth' not in results_df.columns:\n            logger.warning(\"No ground truth available for evaluation\")\n            return {}\n        \n        # Extract predictions and ground truth\n        y_true = results_df['ground_truth'].astype(bool).values\n        y_pred = results_df['ensemble_result'].apply(lambda x: x['final_prediction']).values\n        y_conf = results_df['ensemble_result'].apply(lambda x: x['confidence']).values\n        \n        # Calculate metrics\n        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n        \n        try:\n            auc = roc_auc_score(y_true, y_conf)\n        except ValueError:\n            auc = 0.5 # Default AUC if only one class is present\n        \n        # Per-method analysis\n        method_performance = {}\n        all_methods = ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n        for method in all_methods:\n            method_results = []\n            for _, row in results_df.iterrows():\n                # Find the result for the current method in this row\n                individual_result = next((ir for ir in row['individual_results'] if ir['method_name'] == method), None)\n                if individual_result:\n                    method_results.append({\n                        'prediction': individual_result['is_hallucination'],\n                        'confidence': individual_result['confidence'],\n                        'time': individual_result['execution_time'],\n                        'ground_truth': row['ground_truth']\n                    })\n            \n            if method_results:\n                method_df = pd.DataFrame(method_results)\n                method_y_true = method_df['ground_truth'].astype(bool).values\n                method_y_pred = method_df['prediction'].values\n                \n                method_prec, method_rec, method_f1, _ = precision_recall_fscore_support(\n                    method_y_true, method_y_pred, average='binary', zero_division=0\n                )\n                \n                method_performance[method] = {\n                    'precision': method_prec,\n                    'recall': method_rec,\n                    'f1': method_f1,\n                    'avg_time': method_df['time'].mean(),\n                    'avg_confidence': method_df['confidence'].mean()\n                }\n        \n        return {\n            'ensemble_performance': {\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'auc': auc,\n                'accuracy': (y_pred == y_true).mean()\n            },\n            'method_performance': method_performance,\n            'system_metrics': {\n                'avg_processing_time': results_df['execution_time'].mean(),\n                'total_queries': len(results_df),\n                'consensus_rate': results_df['ensemble_result'].apply(\n                    lambda x: x['consensus_level']\n                ).mean()\n            }\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:52.008586Z","iopub.execute_input":"2025-09-14T21:43:52.008824Z","iopub.status.idle":"2025-09-14T21:43:52.032931Z","shell.execute_reply.started":"2025-09-14T21:43:52.008801Z","shell.execute_reply":"2025-09-14T21:43:52.032383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MAIN EXECUTION AND BENCHMARKING","metadata":{}},{"cell_type":"markdown","source":"## Initialize the complete HELIOS system","metadata":{}},{"cell_type":"code","source":"def setup_helios_system():\n    \"\"\"Initialize the complete HELIOS system\"\"\"\n    print(\"=== Setting up HELIOS   System ===\")\n    \n    try:\n        # Use a hardcoded token for reproducibility in this environment\n        os.environ[\"HF_TOKEN\"] = \"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\"   \n        \n        print(f\"Loading {MODEL_NAME}...\")\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"left\"\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n        )\n        \n        print(\"Loading embedding model...\")\n        embedding_model = SentenceTransformer('all-MiniLM-L6-v2', \n                                            device='cuda' if torch.cuda.is_available() else 'cpu')\n        \n        print(\"✅ All models loaded successfully!\")\n        \n        # Initialize HELIOS\n        helios = HeliosCore(model, tokenizer, embedding_model)\n        \n        return helios\n        \n    except Exception as e:\n        print(f\"❌ Error setting up HELIOS: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:52.034527Z","iopub.execute_input":"2025-09-14T21:43:52.034818Z","iopub.status.idle":"2025-09-14T21:43:52.056648Z","shell.execute_reply.started":"2025-09-14T21:43:52.034773Z","shell.execute_reply":"2025-09-14T21:43:52.056179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## comprehensive HELIOS benchmark","metadata":{}},{"cell_type":"code","source":"def run_helios_benchmark(helios: HeliosCore, input_file: str, output_file: str):\n    \"\"\"\n    Run comprehensive HELIOS benchmark\n    Combines friend's original analysis with your architectural enhancements\n    \"\"\"\n    print(f\"=== Running HELIOS Benchmark on {input_file} ===\")\n    \n    try:\n        # Load test data\n        df = pd.read_csv(input_file)\n        df.dropna(subset=['question', 'answer', 'hallucination_label'], inplace=True)\n        df['hallucination_label'] = df['hallucination_label'].astype(str).str.upper() == 'TRUE'\n        \n        # Convert to list format for batch processing\n        test_data = df.to_dict('records')\n        \n        print(f\"Processing {len(test_data)} examples...\")\n        \n        # Process all modes for comprehensive comparison\n        results = {}\n        \n        for mode in ProcessingMode:\n            print(f\"\\n--- Processing with {mode.value} mode ---\")\n            \n            mode_results = []\n            for item in tqdm(test_data, desc=f\"{mode.value}\"):\n                result = helios.process_query(\n                    item['question'], \n                    item['answer'], \n                    mode=mode\n                )\n                result['ground_truth'] = item['hallucination_label']\n                mode_results.append(result)\n            \n            results[mode.value] = pd.DataFrame(mode_results)\n        \n        # Evaluate all modes\n        print(\"\\n=== HELIOS Performance Analysis ===\")\n        \n        evaluation_results = {}\n        for mode_name, mode_df in results.items():\n            print(f\"\\n--- RESULTS FOR: {mode_name.upper()} MODE ---\")\n            performance = helios.evaluate_performance(mode_df)\n            evaluation_results[mode_name] = performance\n            \n            # --- ENSEMBLE PERFORMANCE ---\n            if 'ensemble_performance' in performance:\n                ens_perf = performance['ensemble_performance']\n                print(\"\\nEnsemble Performance:\")\n                print(f\"  - F1 Score:   {ens_perf['f1']:.3f}\")\n                print(f\"  - Precision:  {ens_perf['precision']:.3f}\")\n                print(f\"  - Recall:     {ens_perf['recall']:.3f}\")\n                print(f\"  - AUC:        {ens_perf['auc']:.3f}\")\n            \n            # =================== FIX 2: ADDED PER-TECHNIQUE REPORTING ===================\n            print(\"\\nPer-Technique Performance:\")\n            if 'method_performance' in performance and performance['method_performance']:\n                perf_data = []\n                for method, metrics in performance['method_performance'].items():\n                    perf_data.append({\n                        'Method': method,\n                        'F1-Score': metrics.get('f1', 0.0),\n                        'Precision': metrics.get('precision', 0.0),\n                        'Recall': metrics.get('recall', 0.0),\n                        'Avg Time (s)': metrics.get('avg_time', 0.0)\n                    })\n                \n                perf_df = pd.DataFrame(perf_data)\n                if not perf_df.empty:\n                    print(perf_df.to_string(index=False, float_format=\"%.3f\"))\n                else:\n                    print(\"  No per-technique data to display.\")\n            else:\n                print(\"  No per-technique performance data available.\")\n            # ===========================================================================\n\n            # --- SYSTEM METRICS ---\n            if 'system_metrics' in performance:\n                sys_metrics = performance['system_metrics']\n                print(\"\\nSystem Metrics:\")\n                print(f\"  - Avg Processing Time: {sys_metrics['avg_processing_time']:.2f}s\")\n                print(f\"  - Avg Consensus Rate:  {sys_metrics['consensus_rate']:.3f}\")\n            print(\"-\" * 50)\n        \n        # Save detailed results\n        comprehensive_results = {\n            'detailed_results': {mode: df.to_dict('records') for mode, df in results.items()},\n            'performance_analysis': evaluation_results,\n            'system_configuration': {\n                'model_name': MODEL_NAME, # This now works because MODEL_NAME is global\n                'processing_modes': [mode.value for mode in ProcessingMode],\n                'detection_methods': ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n            },\n            'execution_metadata': {\n                'timestamp': datetime.now().isoformat(),\n                'total_examples': len(test_data),\n                'framework_version': 'HELIOS  '\n            }\n        }\n        \n        # Save to multiple formats for analysis\n        json_output_file = output_file.replace('.csv', '_comprehensive.json')\n        with open(json_output_file, 'w') as f:\n            json.dump(comprehensive_results, f, indent=2, default=str)\n        \n        # Save simplified CSV for easy analysis\n        simplified_results = []\n        for mode_name, mode_df in results.items():\n            for _, row in mode_df.iterrows():\n                simplified_row = {\n                    'processing_mode': mode_name,\n                    'question': row['query'],\n                    'answer': row['answer'],\n                    'ground_truth': row['ground_truth'],\n                    'ensemble_prediction': row['ensemble_result']['final_prediction'],\n                    'ensemble_confidence': row['ensemble_result']['confidence'],\n                }\n                \n                # Add individual method results\n                for individual in row['individual_results']:\n                    method_name = individual['method_name'].lower()\n                    simplified_row[f'{method_name}_prediction'] = individual['is_hallucination']\n                    simplified_row[f'{method_name}_confidence'] = individual['confidence']\n                \n                simplified_results.append(simplified_row)\n        \n        results_df = pd.DataFrame(simplified_results)\n        results_df.to_csv(output_file, index=False)\n        \n        print(f\"\\n✅ Comprehensive results saved to:\")\n        print(f\"   - Detailed JSON: {json_output_file}\")\n        print(f\"   - Analysis CSV:  {output_file}\")\n        \n        return comprehensive_results\n        \n    except Exception as e:\n        print(f\"❌ Error during benchmark: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:52.057280Z","iopub.execute_input":"2025-09-14T21:43:52.057483Z","iopub.status.idle":"2025-09-14T21:43:52.081682Z","shell.execute_reply.started":"2025-09-14T21:43:52.057468Z","shell.execute_reply":"2025-09-14T21:43:52.081125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adaptive Helios Benchmark","metadata":{}},{"cell_type":"code","source":"def run_helios_adaptive(helios: HeliosCore, input_file: str, output_file: str):\n    \"\"\"\n    Run HELIOS in adaptive mode, choosing the best path for each query.\n    \"\"\"\n    print(f\"=== Running HELIOS in Adaptive Mode on {input_file} ===\")\n    \n    try:\n        # Load test data\n        df = pd.read_csv(input_file)\n        df.dropna(subset=['question', 'answer', 'hallucination_label'], inplace=True)\n        df['hallucination_label'] = df['hallucination_label'].astype(str).str.upper() == 'TRUE'\n        \n        test_data = df.to_dict('records')\n        print(f\"Processing {len(test_data)} examples adaptively...\")\n        \n        adaptive_results = []\n        # Process each item ONCE, without forcing a mode\n        for item in tqdm(test_data, desc=\"Adaptive Processing\"):\n            result = helios.process_query(\n                item['question'], \n                item['answer']\n                # 'mode' is NOT specified, allowing the router to decide\n            )\n            result['ground_truth'] = item['hallucination_label']\n            adaptive_results.append(result)\n        \n        # Save the results to a new file\n        results_df = pd.DataFrame(adaptive_results)\n        \n        # Create a simplified version for easy viewing\n        simplified_df = results_df[['query', 'answer', 'ground_truth', 'processing_mode']].copy()\n        simplified_df['prediction'] = results_df['ensemble_result'].apply(lambda x: x.get('final_prediction'))\n        simplified_df['confidence'] = results_df['ensemble_result'].apply(lambda x: x.get('confidence'))\n        \n        simplified_df.to_csv(output_file, index=False)\n        print(f\"\\n✅ Adaptive processing complete. Simplified results saved to: {output_file}\")\n        \n        # Optionally, run evaluation on these results\n        print(\"\\n--- ADAPTIVE MODE PERFORMANCE ---\")\n        performance = helios.evaluate_performance(results_df)\n        if 'ensemble_performance' in performance:\n            ens_perf = performance['ensemble_performance']\n            print(f\"  - F1 Score:  {ens_perf['f1']:.3f}\")\n            print(f\"  - Precision: {ens_perf['precision']:.3f}\")\n            print(f\"  - Recall:    {ens_perf['recall']:.3f}\")\n\n    except Exception as e:\n        print(f\"❌ Error during adaptive run: {e}\")\n        import traceback\n        traceback.print_exc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:43:52.082503Z","iopub.execute_input":"2025-09-14T21:43:52.082777Z","iopub.status.idle":"2025-09-14T21:43:52.112270Z","shell.execute_reply.started":"2025-09-14T21:43:52.082752Z","shell.execute_reply":"2025-09-14T21:43:52.111764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MAIN EXECUTION FUNCTION","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main execution function that demonstrates the complete HELIOS system\n    \"\"\"\n    print(\"╔════════════════════════════════════════════╗\")\n    print(\"║        HELIOS   EXECUTION               ║\") \n    print(\"║   Unified Hallucination Framework          ║\")\n    print(\"╚════════════════════════════════════════════╝\")\n    \n    # Initialize HELIOS system\n    helios = setup_helios_system()\n    if not helios:\n        print(\"Failed to initialize HELIOS. Exiting.\")\n        return\n\n    # Configuration for dataset path\n    input_file = '/kaggle/input/300dataset/Heios dataset 300 - Sheet1.csv'\n    output_dir = './'\n    \n    # Create dummy file if it doesn't exist for testing\n    if not os.path.exists(input_file):\n        print(f\"Input file not found. Creating a dummy file at '{input_file}'\")\n        dummy_data = {\n            'question': [\"Is it true that the Amazon River is the longest river in the world and that it flows through Brazil, Colombia, and Peru?\", \"Analyze why the Renaissance began in Italy and not in Spain, considering the economic impact of trade routes.?\", \"Tell me about the habitat of the red panda.\"],\n            'answer': [\"Yes, the Amazon River is widely considered the longest river, surpassing the Nile. It originates in the Andes Mountains in Peru and flows through Colombia and Brazil before emptying into the Atlantic Ocean.\", \"The Renaissance began in Italy due to a confluence of factors. Economically, Italian city-states like Venice and Florence dominated Mediterranean trade routes, creating immense wealth and a powerful merchant class that became patrons of the arts...\", \"The red panda lives in temperate forests in the eastern Himalayas and southwestern China.\"],\n            'hallucination_label': [False, True, False]\n        }\n        pd.DataFrame(dummy_data).to_csv(input_file, index=False)\n\n    # Define the output file for the adaptive results\n    adaptive_output_file = os.path.join(output_dir, 'helios_adaptive_results4.csv')\n    \n    # Run the adaptive processing function\n    print(\"\\n\" + \"=\"*60)\n    print(\"PHASE 1: ADAPTIVE PROCESSING (SINGLE PATH PER QUERY)\")\n    print(\"=\"*60)\n    run_helios_adaptive(helios, input_file, adaptive_output_file)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"HELIOS   ADAPTIVE EXECUTION COMPLETE\")\n    print(\"=\"*60)\n\n\n# Execute main function when script is run\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-09-14T21:49:43.097416Z","iopub.execute_input":"2025-09-14T21:49:43.098053Z","iopub.status.idle":"2025-09-14T22:07:02.979630Z","shell.execute_reply.started":"2025-09-14T21:49:43.098023Z","shell.execute_reply":"2025-09-14T22:07:02.979006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}