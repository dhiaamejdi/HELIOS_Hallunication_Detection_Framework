{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT TOKEN SETTINGS AND INSTALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:31.760057Z",
     "iopub.status.busy": "2025-09-26T15:32:31.759261Z",
     "iopub.status.idle": "2025-09-26T15:32:31.764331Z",
     "shell.execute_reply": "2025-09-26T15:32:31.763626Z",
     "shell.execute_reply.started": "2025-09-26T15:32:31.760025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os; os.environ[\"HF_TOKEN\"] =\"\"   \n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "os.environ[\"SERPAPI_API_KEY\"] =\"\" \n",
    "serpapi=os.environ.get(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:31.766090Z",
     "iopub.status.busy": "2025-09-26T15:32:31.765739Z",
     "iopub.status.idle": "2025-09-26T15:32:31.839488Z",
     "shell.execute_reply": "2025-09-26T15:32:31.838934Z",
     "shell.execute_reply.started": "2025-09-26T15:32:31.766062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged into Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Use the secret you created\n",
    "import os\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Authenticate\n",
    "if hf_token:\n",
    "    api = HfApi()\n",
    "    api.whoami(token=hf_token)\n",
    "    print(\"Successfully logged into Hugging Face.\")\n",
    "else:\n",
    "    print(\"Hugging Face token not found. Please add it as a Kaggle Secret with the label 'HF_TOKEN'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:31.840262Z",
     "iopub.status.busy": "2025-09-26T15:32:31.840079Z",
     "iopub.status.idle": "2025-09-26T15:32:35.051348Z",
     "shell.execute_reply": "2025-09-26T15:32:35.050518Z",
     "shell.execute_reply.started": "2025-09-26T15:32:31.840248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: serpapi in /usr/local/lib/python3.11/dist-packages (0.1.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from serpapi) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->serpapi) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.054035Z",
     "iopub.status.busy": "2025-09-26T15:32:35.053576Z",
     "iopub.status.idle": "2025-09-26T15:32:35.060077Z",
     "shell.execute_reply": "2025-09-26T15:32:35.059315Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.054008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Core ML libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# External API libraries\n",
    "import serpapi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.061276Z",
     "iopub.status.busy": "2025-09-26T15:32:35.061010Z",
     "iopub.status.idle": "2025-09-26T15:32:35.078395Z",
     "shell.execute_reply": "2025-09-26T15:32:35.077686Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.061252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='urllib3')\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA STRUCTURES AND ENUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.079490Z",
     "iopub.status.busy": "2025-09-26T15:32:35.079197Z",
     "iopub.status.idle": "2025-09-26T15:32:35.098187Z",
     "shell.execute_reply": "2025-09-26T15:32:35.097651Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.079460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ProcessingMode(Enum):\n",
    "    DETECTION_ONLY = \"detection_only\"\n",
    "    MITIGATION_GUIDED = \"mitigation_guided\"\n",
    "    HYBRID_RESEARCH = \"hybrid_research\"\n",
    "\n",
    "class RoutingStrategy(Enum):\n",
    "    FASTEST = \"fastest\"\n",
    "    MOST_ACCURATE = \"most_accurate\"\n",
    "    COST_EFFECTIVE = \"cost_effective\"\n",
    "    DOMAIN_OPTIMIZED = \"domain_optimized\"\n",
    "\n",
    "@dataclass\n",
    "class QueryFeatures:\n",
    "    \"\"\"Features extracted from a query for routing decisions\"\"\"\n",
    "    query_length: int\n",
    "    entity_count: int\n",
    "    temporal_references: int\n",
    "    domain_category: str\n",
    "    complexity_score: float\n",
    "    factual_density: float\n",
    "    confidence_level: float\n",
    "\n",
    "@dataclass\n",
    "class DetectionResult:\n",
    "    \"\"\"Result from a hallucination detection method\"\"\"\n",
    "    method_name: str\n",
    "    is_hallucination: bool\n",
    "    confidence: float\n",
    "    execution_time: float\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class EnsembleResult:\n",
    "    \"\"\"Final result from ensemble processing\"\"\"\n",
    "    final_prediction: bool\n",
    "    confidence: float\n",
    "    method_votes: Dict[str, bool]\n",
    "    method_confidences: Dict[str, float]\n",
    "    consensus_level: float\n",
    "    processing_path: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORE DETECTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.099273Z",
     "iopub.status.busy": "2025-09-26T15:32:35.099053Z",
     "iopub.status.idle": "2025-09-26T15:32:35.136146Z",
     "shell.execute_reply": "2025-09-26T15:32:35.135610Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.099248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DetectionMethods:\n",
    "    \"\"\"\n",
    "    Container for your friend's core detection implementations\n",
    "    Enhanced with timing and confidence scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, \n",
    "                 embedding_model: SentenceTransformer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Initialize API keys\n",
    "        try:\n",
    "            # In a non-Kaggle environment, you would use os.environ.get(\"SERPAPI_API_KEY\")\n",
    "            self.serp_api_key = \"b913c2d177f219d617cfba13802e7f7a312d8e3a1cd45fec9c2c2e09e54f21cc\" #UserSecretsClient().get_secret(\"SERPAPI_API_KEY\")\n",
    "        except:\n",
    "            self.serp_api_key = None\n",
    "            logger.warning(\"SERPAPI_API_KEY not found. Web search will be disabled.\")\n",
    "\n",
    "    def call_llm_batch(self, prompts: List[List[Dict[str, str]]], \n",
    "                      model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> List[str]:\n",
    "        \"\"\"Friend's batch inference implementation\"\"\"\n",
    "        if not model or not tokenizer:\n",
    "            logger.error(\"Model or tokenizer not loaded\")\n",
    "            return [\"\"] * len(prompts)\n",
    "\n",
    "        # Apply chat template\n",
    "        formatted_prompts = [\n",
    "            tokenizer.apply_chat_template(p, tokenize=False, add_generation_prompt=True)\n",
    "            for p in prompts\n",
    "        ]\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        ).to(model.device)\n",
    "\n",
    "        # Generate\n",
    "        terminators = [\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "        # Decode\n",
    "        response_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs['input_ids'].shape[-1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        return [text.strip() for text in response_texts]\n",
    "\n",
    "    def extract_python_list_from_string(self, s: str) -> list:\n",
    "        \"\"\"Friend's list parsing utility\"\"\"\n",
    "        match = re.search(r'\\[.*\\]', s, re.DOTALL)\n",
    "        if match:\n",
    "            list_str = match.group(0)\n",
    "            try:\n",
    "                return ast.literal_eval(list_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def get_wikidata_entities(self, entity_name: str, limit: int = 5) -> list:\n",
    "        \"\"\"Friend's Wikidata helper\"\"\"\n",
    "        url = \"https://www.wikidata.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"format\": \"json\", \n",
    "            \"language\": \"en\",\n",
    "            \"search\": entity_name,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, params=params, \n",
    "                                  headers={'User-Agent': 'HELIOS/1.0'}, \n",
    "                                  timeout=5)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"search\", [])\n",
    "        except requests.RequestException:\n",
    "            return []\n",
    "\n",
    "    def get_wikidata_triples(self, entity_id: str) -> list:\n",
    "        \"\"\"Friend's Wikidata SPARQL query\"\"\"\n",
    "        endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "        query = f\"\"\"\n",
    "        SELECT ?propLabel ?valueLabel WHERE {{\n",
    "            wd:{entity_id} ?p ?statement .\n",
    "            ?property wikibase:directClaim ?p .\n",
    "            ?property rdfs:label ?propLabel .\n",
    "            ?statement ?ps ?value .\n",
    "            ?value rdfs:label ?valueLabel .\n",
    "            FILTER(LANG(?propLabel) = \"en\")\n",
    "            FILTER(LANG(?valueLabel) = \"en\")\n",
    "        }} LIMIT 25\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(endpoint_url, \n",
    "                                  params={'query': query, 'format': 'json'},\n",
    "                                  headers={'User-Agent': 'HELIOS/1.0'}, \n",
    "                                  timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            triples = [(item.get(\"propLabel\", {}).get(\"value\"), \n",
    "                       item.get(\"valueLabel\", {}).get(\"value\"))\n",
    "                      for item in data.get(\"results\", {}).get(\"bindings\", [])]\n",
    "            return list(set(triples))\n",
    "        except requests.RequestException:\n",
    "            return []\n",
    "\n",
    "    def perform_web_search(self, query: str) -> str:\n",
    "        \"\"\"Friend's web search implementation\"\"\"\n",
    "        if not self.serp_api_key:\n",
    "            return \"No web search available.\"\n",
    "        \n",
    "        try:\n",
    "            client = serpapi.Client(api_key=self.serp_api_key)\n",
    "            params = {'engine': 'google', 'q': query}\n",
    "            results_dict = client.search(**params)\n",
    "            \n",
    "            snippets = []\n",
    "            if \"answer_box\" in results_dict and \"snippet\" in results_dict[\"answer_box\"]:\n",
    "                snippets.append(results_dict[\"answer_box\"][\"snippet\"])\n",
    "            if \"organic_results\" in results_dict:\n",
    "                for result in results_dict[\"organic_results\"][:3]:\n",
    "                    if \"snippet\" in result:\n",
    "                        snippets.append(result[\"snippet\"])\n",
    "            \n",
    "            return \" \".join(snippets) if snippets else \"No relevant information found.\"\n",
    "        except Exception as e:\n",
    "            return f\"Search error: {e}\"\n",
    "\n",
    "    def kgr_detect_hallucination(self, answer: str) -> DetectionResult:\n",
    "        \"\"\"Friend's KGR implementation with enhanced result structure\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract claims\n",
    "        claim_extraction_messages = [[\n",
    "            {\"role\": \"system\", \"content\": \"Extract factual claims from the text. Return only a Python list.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Text: \\\"{answer}\\\"\\nExtracted Claims:\"}\n",
    "        ]]\n",
    "        \n",
    "        claims_response = self.call_llm_batch(claim_extraction_messages, self.model, self.tokenizer)[0]\n",
    "        claims = self.extract_python_list_from_string(claims_response)\n",
    "        \n",
    "        if not claims or not isinstance(claims, list):\n",
    "            return DetectionResult(\n",
    "                method_name=\"KGR\",\n",
    "                is_hallucination=False,\n",
    "                confidence=0.0,\n",
    "                execution_time=time.time() - start_time,\n",
    "                metadata={\"error\": \"Could not extract claims\", \"claims\": []}\n",
    "            )\n",
    "\n",
    "        # Process each claim\n",
    "        verified_claims = 0\n",
    "        total_claims = len(claims)\n",
    "        hallucination_detected = False\n",
    "\n",
    "        for claim in claims:\n",
    "            # Extract entities\n",
    "            entity_messages = [[\n",
    "                {\"role\": \"system\", \"content\": \"Extract key entities. Return only a Python list.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Claim: \\\"{claim}\\\"\\nEntities:\"}\n",
    "            ]]\n",
    "            entity_response = self.call_llm_batch(entity_messages, self.model, self.tokenizer)[0]\n",
    "            entities = self.extract_python_list_from_string(entity_response)\n",
    "            \n",
    "            if not entities:\n",
    "                continue\n",
    "\n",
    "            # Get knowledge graph data\n",
    "            all_triples = []\n",
    "            for entity_name in entities:\n",
    "                search_results = self.get_wikidata_entities(entity_name, limit=1)\n",
    "                if search_results:\n",
    "                    triples = self.get_wikidata_triples(search_results[0]['id'])\n",
    "                    all_triples.extend([(entity_name, prop, val) for prop, val in triples if prop and val])\n",
    "\n",
    "            # Verify claim against knowledge graph\n",
    "            if all_triples:\n",
    "                relevant_facts = \"\\n\".join([f\"- {' '.join(fact)}\" for fact in all_triples])\n",
    "                verification_messages = [[\n",
    "                    {\"role\": \"system\", \"content\": \"Based only on the evidence, is the claim True or False? Answer with one word.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Evidence:\\n{relevant_facts}\\n\\nClaim: \\\"{claim}\\\"\\nVerdict:\"}\n",
    "                ]]\n",
    "                verdict = self.call_llm_batch(verification_messages, self.model, self.tokenizer)[0]\n",
    "                \n",
    "                if 'false' in verdict.lower():\n",
    "                    hallucination_detected = True\n",
    "                    break\n",
    "                verified_claims += 1\n",
    "\n",
    "        confidence = verified_claims / total_claims if total_claims > 0 else 0.0\n",
    "        \n",
    "        return DetectionResult(\n",
    "            method_name=\"KGR\",\n",
    "            is_hallucination=hallucination_detected,\n",
    "            confidence=1.0 - confidence if hallucination_detected else confidence,\n",
    "            execution_time=time.time() - start_time,\n",
    "            metadata={\"claims_processed\": total_claims, \"verified_claims\": verified_claims, \"claims\": claims}\n",
    "        )\n",
    "\n",
    "    def stitch_detect_hallucination(self, answer: str) -> DetectionResult:\n",
    "        \"\"\"Friend's Stitch in Time implementation\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract concepts\n",
    "        concept_extraction_messages = [[\n",
    "            {\"role\": \"system\", \"content\": \"Extract key verifiable concepts. Return comma-separated list.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Sentence: \\\"{answer}\\\"\"}\n",
    "        ]]\n",
    "        concepts_str = self.call_llm_batch(concept_extraction_messages, self.model, self.tokenizer)[0]\n",
    "        \n",
    "        if not concepts_str:\n",
    "            return DetectionResult(\n",
    "                method_name=\"Stitch\",\n",
    "                is_hallucination=False,\n",
    "                confidence=0.0,\n",
    "                execution_time=time.time() - start_time,\n",
    "                metadata={\"error\": \"No concepts extracted\"}\n",
    "            )\n",
    "\n",
    "        concepts = [concept.strip() for concept in concepts_str.split(',')]\n",
    "        verified_concepts = 0\n",
    "        total_concepts = len(concepts)\n",
    "\n",
    "        for concept in concepts:\n",
    "            if not concept:\n",
    "                continue\n",
    "\n",
    "            # Generate validation question\n",
    "            question_gen_messages = [[\n",
    "                {\"role\": \"system\", \"content\": \"Generate a yes/no question to test the concept. Return only the question.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Sentence: \\\"{answer}\\\"\\nConcept: \\\"{concept}\\\"\"}\n",
    "            ]]\n",
    "            validation_question = self.call_llm_batch(question_gen_messages, self.model, self.tokenizer)[0]\n",
    "\n",
    "            if not validation_question:\n",
    "                continue\n",
    "\n",
    "            # Search for evidence\n",
    "            context = self.perform_web_search(validation_question)\n",
    "\n",
    "            # Verify against context\n",
    "            verdict_messages = [[\n",
    "                {\"role\": \"system\", \"content\": \"Based only on search results, answer Yes or No. If unsure, answer No.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Search Results: \\\"{context}\\\"\\nQuestion: \\\"{validation_question}\\\"\"}\n",
    "            ]]\n",
    "            verdict_answer = self.call_llm_batch(verdict_messages, self.model, self.tokenizer)[0].lower()\n",
    "\n",
    "            if \"yes\" in verdict_answer and \"no\" not in verdict_answer:\n",
    "                verified_concepts += 1\n",
    "            else:\n",
    "                # First failure indicates hallucination\n",
    "                return DetectionResult(\n",
    "                    method_name=\"Stitch\",\n",
    "                    is_hallucination=True,\n",
    "                    confidence=0.8,  # High confidence in detection\n",
    "                    execution_time=time.time() - start_time,\n",
    "                    metadata={\"failed_concept\": concept, \"concepts_checked\": len([c for c in concepts[:concepts.index(concept)+1] if c])}\n",
    "                )\n",
    "\n",
    "        confidence = verified_concepts / total_concepts if total_concepts > 0 else 0.0\n",
    "        \n",
    "        return DetectionResult(\n",
    "            method_name=\"Stitch\",\n",
    "            is_hallucination=False,\n",
    "            confidence=confidence,\n",
    "            execution_time=time.time() - start_time,\n",
    "            metadata={\"concepts_verified\": verified_concepts, \"total_concepts\": total_concepts}\n",
    "        )\n",
    "\n",
    "    def interrogate_llm_detect_hallucination(self, question: str, answer: str, \n",
    "                                           k_iterations: int = 5, \n",
    "                                           similarity_threshold: float = 0.91) -> DetectionResult:\n",
    "        \"\"\"Friend's InterrogateLLM implementation\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.embedding_model:\n",
    "            return DetectionResult(\n",
    "                method_name=\"InterrogateLLM\",\n",
    "                is_hallucination=False,\n",
    "                confidence=0.0,\n",
    "                execution_time=time.time() - start_time,\n",
    "                metadata={\"error\": \"Embedding model not available\"}\n",
    "            )\n",
    "\n",
    "        # Generate reconstructed questions\n",
    "        reconstruction_messages = []\n",
    "        for _ in range(k_iterations):\n",
    "            reconstruction_messages.append([\n",
    "                {\"role\": \"system\", \"content\": \"Given this answer, what was the most likely question? Return only the question.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Answer: \\\"{answer}\\\"\\nQuestion:\"}\n",
    "            ])\n",
    "\n",
    "        reconstructed_questions = self.call_llm_batch(reconstruction_messages, self.model, self.tokenizer)\n",
    "\n",
    "        # Calculate similarities\n",
    "        original_embedding = self.embedding_model.encode(question, convert_to_tensor=True)\n",
    "        reconstructed_embeddings = self.embedding_model.encode(reconstructed_questions, convert_to_tensor=True)\n",
    "        \n",
    "        cosine_scores = util.cos_sim(original_embedding, reconstructed_embeddings)\n",
    "        similarity_scores = cosine_scores.flatten().tolist()\n",
    "        average_similarity = np.mean(similarity_scores)\n",
    "\n",
    "        is_hallucination = average_similarity < similarity_threshold\n",
    "        confidence = abs(average_similarity - similarity_threshold) / similarity_threshold\n",
    "\n",
    "        return DetectionResult(\n",
    "            method_name=\"InterrogateLLM\",\n",
    "            is_hallucination=is_hallucination,\n",
    "            confidence=confidence,\n",
    "            execution_time=time.time() - start_time,\n",
    "            metadata={\n",
    "                \"average_similarity\": average_similarity,\n",
    "                \"similarity_scores\": similarity_scores,\n",
    "                \"threshold\": similarity_threshold,\n",
    "                \"reconstructed_questions\": reconstructed_questions\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def cove_detect_hallucination(self, question: str, answer: str) -> DetectionResult:\n",
    "        \"\"\"Friend's Chain-of-Verification implementation\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Plan verification questions\n",
    "        plan_messages = [[\n",
    "            {\"role\": \"system\", \"content\": \"Generate verification questions for fact-checking. Return only a Python list.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: \\\"{question}\\\"\\nAnswer: \\\"{answer}\\\"\"}\n",
    "        ]]\n",
    "        questions_response = self.call_llm_batch(plan_messages, self.model, self.tokenizer)[0]\n",
    "        verification_questions = self.extract_python_list_from_string(questions_response)\n",
    "\n",
    "        if not verification_questions or not isinstance(verification_questions, list):\n",
    "            return DetectionResult(\n",
    "                method_name=\"CoVE\",\n",
    "                is_hallucination=False,\n",
    "                confidence=0.0,\n",
    "                execution_time=time.time() - start_time,\n",
    "                metadata={\"error\": \"Could not generate verification questions\"}\n",
    "            )\n",
    "\n",
    "        # Execute verifications\n",
    "        execution_messages = []\n",
    "        for v_question in verification_questions:\n",
    "            execution_messages.append([\n",
    "                {\"role\": \"system\", \"content\": \"Answer this question factually and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": v_question}\n",
    "            ])\n",
    "\n",
    "        verification_answers = self.call_llm_batch(execution_messages, self.model, self.tokenizer)\n",
    "\n",
    "        # Check for contradictions\n",
    "        contradictions = 0\n",
    "        for i, (v_question, v_answer) in enumerate(zip(verification_questions, verification_answers)):\n",
    "            contradiction_messages = [[\n",
    "                {\"role\": \"system\", \"content\": \"Does the verified fact contradict the original answer? Answer Yes or No.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Original: \\\"{answer}\\\"\\nVerified: '{v_question}' -> '{v_answer}'\\nContradiction?\"}\n",
    "            ]]\n",
    "            verdict = self.call_llm_batch(contradiction_messages, self.model, self.tokenizer)[0]\n",
    "            \n",
    "            if 'yes' in verdict.strip().lower():\n",
    "                contradictions += 1\n",
    "\n",
    "        is_hallucination = contradictions > 0\n",
    "        confidence = contradictions / len(verification_questions) if verification_questions else 0.0\n",
    "\n",
    "        return DetectionResult(\n",
    "            method_name=\"CoVE\",\n",
    "            is_hallucination=is_hallucination,\n",
    "            confidence=confidence if is_hallucination else 1.0 - confidence,\n",
    "            execution_time=time.time() - start_time,\n",
    "            metadata={\n",
    "                \"verification_questions\": verification_questions,\n",
    "                \"contradictions_found\": contradictions,\n",
    "                \"total_verifications\": len(verification_questions)\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHITECTURAL SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced query analysis for intelligent routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.136929Z",
     "iopub.status.busy": "2025-09-26T15:32:35.136721Z",
     "iopub.status.idle": "2025-09-26T15:32:35.162194Z",
     "shell.execute_reply": "2025-09-26T15:32:35.161579Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.136914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QueryAnalyzer:\n",
    "    \"\"\"\n",
    "     Advanced query analysis for intelligent routing (Final, Corrected Version)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We now separate analytical keywords from simple complexity indicators\n",
    "        self.domain_keywords = {\n",
    "            'factual': ['what', 'when', 'where', 'who', 'how many', 'date', 'year', 'is it true'],\n",
    "            'creative': ['write', 'create', 'imagine', 'story', 'poem', 'creative'],\n",
    "            # NEW: A specific list for truly analytical queries\n",
    "            'analytical': ['analyze', 'compare', 'evaluate', 'explain', 'why', 'impact', 'because', 'contrast']\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, query: str, answer: str = \"\") -> QueryFeatures:\n",
    "        \"\"\"Extract comprehensive features for routing decisions with better text cleaning.\"\"\"\n",
    "        \n",
    "        cleaned_query = re.sub(r'[^\\w\\s]', '', query.lower())\n",
    "        tokens = cleaned_query.split()\n",
    "        \n",
    "        query_length = len(tokens)\n",
    "        entity_count = len(re.findall(r'\\b[A-Z][a-z]+(?:\\s[A-Z][a-z]+)*\\b', query))\n",
    "        \n",
    "        temporal_patterns = [r'\\b\\d{4}\\b', r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', r'\\b(?:yesterday|today|tomorrow|last|next)\\b']\n",
    "        temporal_references = sum(len(re.findall(pattern, query, re.IGNORECASE)) for pattern in temporal_patterns)\n",
    "        \n",
    "        # --- MODIFIED: Domain classification now uses the new 'analytical' list ---\n",
    "        domain_scores = {}\n",
    "        for domain, keywords in self.domain_keywords.items():\n",
    "            score = 0\n",
    "            for keyword in keywords:\n",
    "                if ' ' in keyword and keyword in cleaned_query:\n",
    "                    score += 1\n",
    "                elif keyword in tokens:\n",
    "                    score += 1\n",
    "            domain_scores[domain] = score\n",
    "        \n",
    "        # We now prioritize 'analytical' if it appears.\n",
    "        if domain_scores.get('analytical', 0) > 0:\n",
    "            domain_category = 'analytical'\n",
    "        else:\n",
    "            domain_category = max(domain_scores, key=lambda k: domain_scores.get(k, 0)) if any(domain_scores.values()) else 'general'\n",
    "\n",
    "        # --- MODIFIED: Complexity is now just about conjunctions/length, not analysis ---\n",
    "        complexity_indicators = ['and', 'but', 'or']\n",
    "        # A simple measure: is it a long sentence OR does it contain a conjunction?\n",
    "        complexity_score = 1.0 if query_length > 15 or any(ind in tokens for ind in complexity_indicators) else 0.0\n",
    "\n",
    "        factual_indicators = ['is', 'was', 'are', 'were', 'has', 'have', 'did', 'does']\n",
    "        factual_density = min(1.0, sum(1 for indicator in factual_indicators if indicator in tokens) / len(tokens)) if tokens else 0.0\n",
    "        \n",
    "        confidence_level = 0.8\n",
    "        \n",
    "        return QueryFeatures(\n",
    "            query_length=query_length,\n",
    "            entity_count=entity_count,\n",
    "            temporal_references=temporal_references,\n",
    "            domain_category=domain_category,\n",
    "            complexity_score=complexity_score,\n",
    "            factual_density=factual_density,\n",
    "            confidence_level=confidence_level\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent routing system with ML-based decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.163285Z",
     "iopub.status.busy": "2025-09-26T15:32:35.163059Z",
     "iopub.status.idle": "2025-09-26T15:32:35.187587Z",
     "shell.execute_reply": "2025-09-26T15:32:35.186999Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.163258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdaptiveRouter:\n",
    "    \"\"\"\n",
    "     Intelligent routing system with ML-based decisions (Final, Production-Ready Logic)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str] = None):\n",
    "        self.routing_model = None\n",
    "        self.performance_history = {}\n",
    "        self.model_path = model_path\n",
    "        self.query_analyzer = QueryAnalyzer() # Assumes you are using the improved QueryAnalyzer from the previous step\n",
    "        \n",
    "        if model_path and Path(model_path).exists():\n",
    "            self.load_model()\n",
    "        else:\n",
    "            self.routing_model = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    # ... (load_model, save_model are unchanged) ...\n",
    "    def load_model(self):\n",
    "        \"\"\"Load pre-trained routing model\"\"\"\n",
    "        try:\n",
    "            with open(self.model_path, 'rb') as f:\n",
    "                self.routing_model = pickle.load(f)\n",
    "            logger.info(\"Routing model loaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load routing model: {e}\")\n",
    "            self.routing_model = DecisionTreeClassifier(random_state=42)\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save routing model\"\"\"\n",
    "        try:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(self.routing_model, f)\n",
    "            logger.info(f\"Routing model saved to {path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save routing model: {e}\")\n",
    "\n",
    "    # =================== THIS IS THE FINAL, DEFINITIVE VERSION ===================\n",
    "    def decide_processing_mode(self, query: str, answer: str = \"\") -> ProcessingMode:\n",
    "        \"\"\"\n",
    "        Decide which processing path to take using robust, multi-factor rules.\n",
    "        \"\"\"\n",
    "        features = self.query_analyzer.extract_features(query, answer)\n",
    "        \n",
    "        # --- Priority 1: Check for explicitly ANALYTICAL queries ---\n",
    "        # This remains the same. It's for the most complex tasks.\n",
    "        if features.domain_category == 'analytical':\n",
    "            logger.info(f\"Routing to HYBRID_RESEARCH because the query is analytical.\")\n",
    "            return ProcessingMode.HYBRID_RESEARCH\n",
    "\n",
    "        # --- Priority 2: Check for FACTUAL queries with robust signals ---\n",
    "        # We now look for multiple signals of a fact-checking task.\n",
    "        is_short_direct_question = features.query_length < 10 and '?' in query\n",
    "        \n",
    "        # NEW, MORE ROBUST CHECK: Is the statement rich with entities?\n",
    "        # A high number of named entities (people, places, dates) is a very strong\n",
    "        # signal that the text is stating facts. We'll use a threshold of 3 or more.\n",
    "        is_entity_rich = features.entity_count + features.temporal_references >= 3\n",
    "\n",
    "        if features.domain_category == 'factual' and (is_short_direct_question or is_entity_rich):\n",
    "            logger.info(f\"Routing to DETECTION_ONLY. Reason: Factual domain with either high entity count ({is_entity_rich}) or being a short, direct question ({is_short_direct_question}).\")\n",
    "            return ProcessingMode.DETECTION_ONLY\n",
    "\n",
    "        # --- Priority 3: Fallback for all OTHER queries ---\n",
    "        logger.info(f\"Routing to MITIGATION_GUIDED as the default path for domain: {features.domain_category}.\")\n",
    "        return ProcessingMode.MITIGATION_GUIDED\n",
    "    # =====================================================================\n",
    "\n",
    "    # ... (select_detection_methods and update_performance are unchanged) ...\n",
    "    def select_detection_methods(self, query: str, answer: str) -> List[str]:\n",
    "        \"\"\"Select optimal detection methods based on query characteristics\"\"\"\n",
    "        features = self.query_analyzer.extract_features(query, answer)\n",
    "        \n",
    "        # Method selection logic\n",
    "        methods = []\n",
    "        \n",
    "        # Always include baseline methods\n",
    "        if query and answer:  # Q+A methods\n",
    "            methods.extend([\"CoVE\", \"InterrogateLLM\"])\n",
    "        \n",
    "        if answer:  # A-only methods\n",
    "            methods.extend([\"KGR\", \"Stitch\"])\n",
    "        \n",
    "        # Priority-based selection based on query characteristics\n",
    "        if features.entity_count > 2:\n",
    "            methods = [\"KGR\"] + [m for m in methods if m != \"KGR\"]\n",
    "        \n",
    "        if features.temporal_references > 0:\n",
    "            methods = [\"Stitch\"] + [m for m in methods if m != \"Stitch\"]\n",
    "        \n",
    "        if features.complexity_score > 0.7:\n",
    "            methods = [\"CoVE\"] + [m for m in methods if m != \"CoVE\"]\n",
    "        \n",
    "        return list(dict.fromkeys(methods)) # Return unique methods while preserving order\n",
    "    \n",
    "    def update_performance(self, query: str, answer: str, method: str, \n",
    "                          result: DetectionResult, ground_truth: Optional[bool] = None):\n",
    "        \"\"\"Update performance tracking for adaptive learning\"\"\"\n",
    "        features = self.query_analyzer.extract_features(query, answer)\n",
    "        \n",
    "        key = f\"{features.domain_category}_{method}\"\n",
    "        if key not in self.performance_history:\n",
    "            self.performance_history[key] = {\n",
    "                'accuracy_scores': [],\n",
    "                'latency_scores': [],\n",
    "                'confidence_scores': []\n",
    "            }\n",
    "        \n",
    "        # Track performance metrics\n",
    "        if ground_truth is not None:\n",
    "            accuracy = 1.0 if result.is_hallucination == ground_truth else 0.0\n",
    "            self.performance_history[key]['accuracy_scores'].append(accuracy)\n",
    "        \n",
    "        self.performance_history[key]['latency_scores'].append(result.execution_time)\n",
    "        self.performance_history[key]['confidence_scores'].append(result.confidence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced ensemble fusion algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.190091Z",
     "iopub.status.busy": "2025-09-26T15:32:35.189498Z",
     "iopub.status.idle": "2025-09-26T15:32:35.216827Z",
     "shell.execute_reply": "2025-09-26T15:32:35.216239Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.190067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnsembleFusion:\n",
    "    \"\"\"\n",
    "     Advanced ensemble fusion algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.method_weights = {\n",
    "            'KGR': 1.0,\n",
    "            'CoVE': 1.0, \n",
    "            'Stitch': 1.0,\n",
    "            'InterrogateLLM': 1.0\n",
    "        }\n",
    "        self.domain_weights = {}\n",
    "    \n",
    "    def weighted_voting(self, results: List[DetectionResult]) -> EnsembleResult:\n",
    "        \"\"\"Weighted voting based on historical performance\"\"\"\n",
    "        if not results:\n",
    "            return EnsembleResult(\n",
    "                final_prediction=False,\n",
    "                confidence=0.0,\n",
    "                method_votes={},\n",
    "                method_confidences={},\n",
    "                consensus_level=0.0,\n",
    "                processing_path=\"error\"\n",
    "            )\n",
    "        \n",
    "        votes = {}\n",
    "        confidences = {}\n",
    "        weighted_sum = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for result in results:\n",
    "            method = result.method_name\n",
    "            weight = self.method_weights.get(method, 1.0)\n",
    "            \n",
    "            votes[method] = result.is_hallucination\n",
    "            confidences[method] = result.confidence\n",
    "            \n",
    "            # Weight the vote by method performance and confidence\n",
    "            vote_value = 1.0 if result.is_hallucination else 0.0\n",
    "            weighted_sum += weight * vote_value * result.confidence\n",
    "            total_weight += weight * result.confidence\n",
    "        \n",
    "        # Final prediction\n",
    "        final_score = weighted_sum / total_weight if total_weight > 0 else 0.0\n",
    "        final_prediction = final_score > 0.5\n",
    "        \n",
    "        # Calculate consensus level\n",
    "        num_results = len(results)\n",
    "        if num_results > 0:\n",
    "            positive_votes = sum(1 for v in votes.values() if v)\n",
    "            consensus_level = abs(positive_votes - num_results / 2) / (num_results / 2) if num_results > 1 else 1.0\n",
    "        else:\n",
    "            consensus_level = 0.0\n",
    "        \n",
    "        return EnsembleResult(\n",
    "            final_prediction=final_prediction,\n",
    "            confidence=final_score if final_prediction else 1.0 - final_score,\n",
    "            method_votes=votes,\n",
    "            method_confidences=confidences,\n",
    "            consensus_level=consensus_level,\n",
    "            processing_path=\"weighted_voting\"\n",
    "        )\n",
    "    \n",
    "    def specialization_routing(self, query: str, results: List[DetectionResult]) -> EnsembleResult:\n",
    "        \"\"\"Route based on method specialization\"\"\"\n",
    "        # Separate Q+A methods from A-only methods\n",
    "        qa_methods = [r for r in results if r.method_name in ['CoVE', 'InterrogateLLM']]\n",
    "        \n",
    "        # If Q+A methods agree, trust them\n",
    "        if len(qa_methods) >= 2:\n",
    "            qa_votes = [r.is_hallucination for r in qa_methods]\n",
    "            if len(set(qa_votes)) == 1:  # All agree\n",
    "                avg_confidence = np.mean([r.confidence for r in qa_methods])\n",
    "                return EnsembleResult(\n",
    "                    final_prediction=qa_votes[0],\n",
    "                    confidence=avg_confidence,\n",
    "                    method_votes={r.method_name: r.is_hallucination for r in results},\n",
    "                    method_confidences={r.method_name: r.confidence for r in results},\n",
    "                    consensus_level=1.0,\n",
    "                    processing_path=\"qa_consensus\"\n",
    "                )\n",
    "        \n",
    "        # Fall back to weighted voting\n",
    "        return self.weighted_voting(results)\n",
    "    \n",
    "    def confidence_fusion(self, results: List[DetectionResult], \n",
    "                         confidence_threshold: float = 0.8) -> EnsembleResult:\n",
    "        \"\"\"Confidence-aware fusion that can abstain from low-confidence decisions\"\"\"\n",
    "        \n",
    "        # Filter high-confidence results\n",
    "        high_confidence_results = [r for r in results if r.confidence >= confidence_threshold]\n",
    "        \n",
    "        if not high_confidence_results:\n",
    "            # No high-confidence results, return uncertain by falling back to standard voting\n",
    "            return self.weighted_voting(results)\n",
    "        \n",
    "        # Use only high-confidence results for decision\n",
    "        return self.weighted_voting(high_confidence_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main orchestration system that brings everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.217859Z",
     "iopub.status.busy": "2025-09-26T15:32:35.217598Z",
     "iopub.status.idle": "2025-09-26T15:32:35.236441Z",
     "shell.execute_reply": "2025-09-26T15:32:35.235836Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.217815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HeliosCore:\n",
    "    \"\"\"\n",
    "     Main orchestration system that brings everything together\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: AutoModelForCausalLM, tokenizer: AutoTokenizer,\n",
    "                 embedding_model: SentenceTransformer):\n",
    "        \n",
    "        # Core components\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        \n",
    "        # Your architectural components\n",
    "        self.router = AdaptiveRouter()\n",
    "        self.fusion_engine = EnsembleFusion()\n",
    "        self.query_analyzer = QueryAnalyzer()\n",
    "        \n",
    "        # Friend's detection methods\n",
    "        self.detection_methods = DetectionMethods(model, tokenizer, embedding_model)\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.execution_logs = []\n",
    "        self.performance_metrics = {}\n",
    "    \n",
    "    def process_query(self, query: str, answer: str, \n",
    "                     mode: Optional[ProcessingMode] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main processing pipeline that orchestrates the entire HELIOS system\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Stage 0: Adaptive Routing\n",
    "        if mode is None:\n",
    "            mode = self.router.decide_processing_mode(query, answer)\n",
    "        print(mode)\n",
    "        selected_methods = self.router.select_detection_methods(query, answer)\n",
    "        \n",
    "        logger.info(f\"Processing query with mode: {mode.value}\")\n",
    "        logger.info(f\"Selected methods: {selected_methods}\")\n",
    "        \n",
    "        # Stage 1: Parallel Detection Pipeline\n",
    "        detection_results = []\n",
    "        \n",
    "        for method_name in selected_methods:\n",
    "            try:\n",
    "                if method_name == \"KGR\":\n",
    "                    result = self.detection_methods.kgr_detect_hallucination(answer)\n",
    "                elif method_name == \"CoVE\":\n",
    "                    result = self.detection_methods.cove_detect_hallucination(query, answer)\n",
    "                elif method_name == \"Stitch\":\n",
    "                    result = self.detection_methods.stitch_detect_hallucination(answer)\n",
    "                elif method_name == \"InterrogateLLM\":\n",
    "                    result = self.detection_methods.interrogate_llm_detect_hallucination(query, answer)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                detection_results.append(result)\n",
    "                logger.info(f\"{method_name}: {result.is_hallucination} (conf: {result.confidence:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {method_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Stage 2: Intelligent Ensemble Fusion\n",
    "        if mode == ProcessingMode.DETECTION_ONLY:\n",
    "            ensemble_result = self.fusion_engine.weighted_voting(detection_results)\n",
    "        elif mode == ProcessingMode.HYBRID_RESEARCH:\n",
    "            ensemble_result = self.fusion_engine.specialization_routing(query, detection_results)\n",
    "        else:  # MITIGATION_GUIDED\n",
    "            ensemble_result = self.fusion_engine.confidence_fusion(detection_results)\n",
    "        \n",
    "        # Compile comprehensive results\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'processing_mode': mode.value,\n",
    "            'selected_methods': selected_methods,\n",
    "            'individual_results': [asdict(r) for r in detection_results],\n",
    "            'ensemble_result': asdict(ensemble_result),\n",
    "            'execution_time': total_time,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Log for continuous learning\n",
    "        self.execution_logs.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_process(self, data: List[Dict[str, str]], \n",
    "                     ground_truth_col: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process multiple queries in batch for benchmarking\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for item in tqdm(data, desc=\"Processing batch\"):\n",
    "            query = item.get('question', '')\n",
    "            answer = item.get('answer', '')\n",
    "            ground_truth = item.get(ground_truth_col) if ground_truth_col else None\n",
    "            \n",
    "            result = self.process_query(query, answer)\n",
    "            \n",
    "            # Add ground truth for evaluation\n",
    "            if ground_truth is not None:\n",
    "                result['ground_truth'] = ground_truth\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def evaluate_performance(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "         Comprehensive evaluation framework\n",
    "        \"\"\"\n",
    "        if 'ground_truth' not in results_df.columns:\n",
    "            logger.warning(\"No ground truth available for evaluation\")\n",
    "            return {}\n",
    "        \n",
    "        # Extract predictions and ground truth\n",
    "        y_true = results_df['ground_truth'].astype(bool).values\n",
    "        y_pred = results_df['ensemble_result'].apply(lambda x: x['final_prediction']).values\n",
    "        y_conf = results_df['ensemble_result'].apply(lambda x: x['confidence']).values\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_conf)\n",
    "        except ValueError:\n",
    "            auc = 0.5 # Default AUC if only one class is present\n",
    "        \n",
    "        # Per-method analysis\n",
    "        method_performance = {}\n",
    "        all_methods = ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n",
    "        for method in all_methods:\n",
    "            method_results = []\n",
    "            for _, row in results_df.iterrows():\n",
    "                # Find the result for the current method in this row\n",
    "                individual_result = next((ir for ir in row['individual_results'] if ir['method_name'] == method), None)\n",
    "                if individual_result:\n",
    "                    method_results.append({\n",
    "                        'prediction': individual_result['is_hallucination'],\n",
    "                        'confidence': individual_result['confidence'],\n",
    "                        'time': individual_result['execution_time'],\n",
    "                        'ground_truth': row['ground_truth']\n",
    "                    })\n",
    "            \n",
    "            if method_results:\n",
    "                method_df = pd.DataFrame(method_results)\n",
    "                method_y_true = method_df['ground_truth'].astype(bool).values\n",
    "                method_y_pred = method_df['prediction'].values\n",
    "                \n",
    "                method_prec, method_rec, method_f1, _ = precision_recall_fscore_support(\n",
    "                    method_y_true, method_y_pred, average='binary', zero_division=0\n",
    "                )\n",
    "                \n",
    "                method_performance[method] = {\n",
    "                    'precision': method_prec,\n",
    "                    'recall': method_rec,\n",
    "                    'f1': method_f1,\n",
    "                    'avg_time': method_df['time'].mean(),\n",
    "                    'avg_confidence': method_df['confidence'].mean()\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'ensemble_performance': {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'accuracy': (y_pred == y_true).mean()\n",
    "            },\n",
    "            'method_performance': method_performance,\n",
    "            'system_metrics': {\n",
    "                'avg_processing_time': results_df['execution_time'].mean(),\n",
    "                'total_queries': len(results_df),\n",
    "                'consensus_rate': results_df['ensemble_result'].apply(\n",
    "                    lambda x: x['consensus_level']\n",
    "                ).mean()\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION AND BENCHMARKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the complete HELIOS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.237287Z",
     "iopub.status.busy": "2025-09-26T15:32:35.237118Z",
     "iopub.status.idle": "2025-09-26T15:32:35.259252Z",
     "shell.execute_reply": "2025-09-26T15:32:35.258672Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.237273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_helios_system():\n",
    "    \"\"\"Initialize the complete HELIOS system\"\"\"\n",
    "    print(\"=== Setting up HELIOS v2.0 System ===\")\n",
    "    \n",
    "    try:\n",
    "        # Use a hardcoded token for reproducibility in this environment\n",
    "        os.environ[\"HF_TOKEN\"] = \"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\"   \n",
    "        \n",
    "        print(f\"Loading {MODEL_NAME}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        \n",
    "        print(\"Loading embedding model...\")\n",
    "        embedding_model = SentenceTransformer('all-MiniLM-L6-v2', \n",
    "                                            device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(\" All models loaded successfully!\")\n",
    "        \n",
    "        # Initialize HELIOS\n",
    "        helios = HeliosCore(model, tokenizer, embedding_model)\n",
    "        \n",
    "        return helios\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error setting up HELIOS: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comprehensive HELIOS benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.260345Z",
     "iopub.status.busy": "2025-09-26T15:32:35.260093Z",
     "iopub.status.idle": "2025-09-26T15:32:35.284492Z",
     "shell.execute_reply": "2025-09-26T15:32:35.283742Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.260327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_helios_benchmark(helios: HeliosCore, input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Run comprehensive HELIOS benchmark\n",
    "    Combines friend's original analysis with your architectural enhancements\n",
    "    \"\"\"\n",
    "    print(f\"=== Running HELIOS Benchmark on {input_file} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load test data\n",
    "        df = pd.read_csv(input_file)\n",
    "        df.dropna(subset=['question', 'answer', 'hallucination_label'], inplace=True)\n",
    "        df['hallucination_label'] = df['hallucination_label'].astype(str).str.upper() == 'TRUE'\n",
    "        \n",
    "        # Convert to list format for batch processing\n",
    "        test_data = df.to_dict('records')\n",
    "        \n",
    "        print(f\"Processing {len(test_data)} examples...\")\n",
    "        \n",
    "        # Process all modes for comprehensive comparison\n",
    "        results = {}\n",
    "        \n",
    "        for mode in ProcessingMode:\n",
    "            print(f\"\\n--- Processing with {mode.value} mode ---\")\n",
    "            \n",
    "            mode_results = []\n",
    "            for item in tqdm(test_data, desc=f\"{mode.value}\"):\n",
    "                result = helios.process_query(\n",
    "                    item['question'], \n",
    "                    item['answer'], \n",
    "                    mode=mode\n",
    "                )\n",
    "                result['ground_truth'] = item['hallucination_label']\n",
    "                mode_results.append(result)\n",
    "            \n",
    "            results[mode.value] = pd.DataFrame(mode_results)\n",
    "        \n",
    "        # Evaluate all modes\n",
    "        print(\"\\n=== HELIOS Performance Analysis ===\")\n",
    "        \n",
    "        evaluation_results = {}\n",
    "        for mode_name, mode_df in results.items():\n",
    "            print(f\"\\n--- RESULTS FOR: {mode_name.upper()} MODE ---\")\n",
    "            performance = helios.evaluate_performance(mode_df)\n",
    "            evaluation_results[mode_name] = performance\n",
    "            \n",
    "            # --- ENSEMBLE PERFORMANCE ---\n",
    "            if 'ensemble_performance' in performance:\n",
    "                ens_perf = performance['ensemble_performance']\n",
    "                print(\"\\nEnsemble Performance:\")\n",
    "                print(f\"  - F1 Score:   {ens_perf['f1']:.3f}\")\n",
    "                print(f\"  - Precision:  {ens_perf['precision']:.3f}\")\n",
    "                print(f\"  - Recall:     {ens_perf['recall']:.3f}\")\n",
    "                print(f\"  - AUC:        {ens_perf['auc']:.3f}\")\n",
    "            \n",
    "            # =================== FIX 2: ADDED PER-TECHNIQUE REPORTING ===================\n",
    "            print(\"\\nPer-Technique Performance:\")\n",
    "            if 'method_performance' in performance and performance['method_performance']:\n",
    "                perf_data = []\n",
    "                for method, metrics in performance['method_performance'].items():\n",
    "                    perf_data.append({\n",
    "                        'Method': method,\n",
    "                        'F1-Score': metrics.get('f1', 0.0),\n",
    "                        'Precision': metrics.get('precision', 0.0),\n",
    "                        'Recall': metrics.get('recall', 0.0),\n",
    "                        'Avg Time (s)': metrics.get('avg_time', 0.0)\n",
    "                    })\n",
    "                \n",
    "                perf_df = pd.DataFrame(perf_data)\n",
    "                if not perf_df.empty:\n",
    "                    print(perf_df.to_string(index=False, float_format=\"%.3f\"))\n",
    "                else:\n",
    "                    print(\"  No per-technique data to display.\")\n",
    "            else:\n",
    "                print(\"  No per-technique performance data available.\")\n",
    "            # ===========================================================================\n",
    "\n",
    "            # --- SYSTEM METRICS ---\n",
    "            if 'system_metrics' in performance:\n",
    "                sys_metrics = performance['system_metrics']\n",
    "                print(\"\\nSystem Metrics:\")\n",
    "                print(f\"  - Avg Processing Time: {sys_metrics['avg_processing_time']:.2f}s\")\n",
    "                print(f\"  - Avg Consensus Rate:  {sys_metrics['consensus_rate']:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        # Save detailed results\n",
    "        comprehensive_results = {\n",
    "            'detailed_results': {mode: df.to_dict('records') for mode, df in results.items()},\n",
    "            'performance_analysis': evaluation_results,\n",
    "            'system_configuration': {\n",
    "                'model_name': MODEL_NAME, # This now works because MODEL_NAME is global\n",
    "                'processing_modes': [mode.value for mode in ProcessingMode],\n",
    "                'detection_methods': ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n",
    "            },\n",
    "            'execution_metadata': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'total_examples': len(test_data),\n",
    "                'framework_version': 'HELIOS v2.0'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to multiple formats for analysis\n",
    "        json_output_file = output_file.replace('.csv', '_comprehensive.json')\n",
    "        with open(json_output_file, 'w') as f:\n",
    "            json.dump(comprehensive_results, f, indent=2, default=str)\n",
    "        \n",
    "        # Save simplified CSV for easy analysis\n",
    "        simplified_results = []\n",
    "        for mode_name, mode_df in results.items():\n",
    "            for _, row in mode_df.iterrows():\n",
    "                simplified_row = {\n",
    "                    'processing_mode': mode_name,\n",
    "                    'question': row['query'],\n",
    "                    'answer': row['answer'],\n",
    "                    'ground_truth': row['ground_truth'],\n",
    "                    'ensemble_prediction': row['ensemble_result']['final_prediction'],\n",
    "                    'ensemble_confidence': row['ensemble_result']['confidence'],\n",
    "                }\n",
    "                \n",
    "                # Add individual method results\n",
    "                for individual in row['individual_results']:\n",
    "                    method_name = individual['method_name'].lower()\n",
    "                    simplified_row[f'{method_name}_prediction'] = individual['is_hallucination']\n",
    "                    simplified_row[f'{method_name}_confidence'] = individual['confidence']\n",
    "                \n",
    "                simplified_results.append(simplified_row)\n",
    "        \n",
    "        results_df = pd.DataFrame(simplified_results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n Comprehensive results saved to:\")\n",
    "        print(f\"   - Detailed JSON: {json_output_file}\")\n",
    "        print(f\"   - Analysis CSV:  {output_file}\")\n",
    "        \n",
    "        return comprehensive_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error during benchmark: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Helios Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.285883Z",
     "iopub.status.busy": "2025-09-26T15:32:35.285448Z",
     "iopub.status.idle": "2025-09-26T15:32:35.306761Z",
     "shell.execute_reply": "2025-09-26T15:32:35.306260Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.285861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_helios_adaptive(helios: HeliosCore, input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Run HELIOS in adaptive mode and save a COMPREHENSIVE, flat CSV with all results.\n",
    "    \"\"\"\n",
    "    print(f\"=== Running HELIOS in Adaptive Mode on {input_file} ===\")\n",
    "    \n",
    "    try:\n",
    "        # Load test data (this part is unchanged)\n",
    "        df = pd.read_csv(input_file)\n",
    "        df.dropna(subset=['question', 'answer', 'hallucination_label'], inplace=True)\n",
    "        df['hallucination_label'] = df['hallucination_label'].astype(str).str.upper() == 'TRUE'\n",
    "        \n",
    "        test_data = df.to_dict('records')\n",
    "        print(f\"Processing {len(test_data)} examples adaptively...\")\n",
    "        \n",
    "        # This part is the same: process each item and get the detailed results\n",
    "        adaptive_results = []\n",
    "        for item in tqdm(test_data, desc=\"Adaptive Processing\"):\n",
    "            result = helios.process_query(\n",
    "                item['question'], \n",
    "                item['answer']\n",
    "            )\n",
    "            result['ground_truth'] = item['hallucination_label']\n",
    "            adaptive_results.append(result)\n",
    "\n",
    "        print(\"Adaptive result\", adaptive_results)\n",
    "        \n",
    "        #  START OF MODIFIED SECTION \n",
    "        # Instead of saving a simplified CSV, we will now \"flatten\" the\n",
    "        # detailed results to create a comprehensive CSV.\n",
    "        \n",
    "        wide_format_results = []\n",
    "        for result in adaptive_results:\n",
    "            # This is the base information, which will be the start of our single row.\n",
    "            row = {\n",
    "                'query': result['query'],\n",
    "                'answer': result['answer'],\n",
    "                'ground_truth': result['ground_truth'],\n",
    "                'processing_mode': result['processing_mode'],\n",
    "                'ensemble_prediction': result['ensemble_result']['final_prediction'],\n",
    "                'ensemble_confidence': result['ensemble_result']['confidence']\n",
    "            }\n",
    "            \n",
    "            # Now, instead of creating new rows, we create new COLUMNS.\n",
    "            # We transform the list of individual results into a dictionary for easy lookup.\n",
    "            method_results_dict = {\n",
    "                res['method_name']: res for res in result.get('individual_results', [])\n",
    "            }\n",
    "\n",
    "            # Iterate through ALL possible methods to create a consistent set of columns.\n",
    "            all_methods = ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n",
    "            for method_name in all_methods:\n",
    "                # Check if this method was run for this query\n",
    "                if method_name in method_results_dict:\n",
    "                    # If it ran, add its results to the corresponding columns.\n",
    "                    method_data = method_results_dict[method_name]\n",
    "                    row[f'{method_name}_prediction'] = method_data['is_hallucination']\n",
    "                    row[f'{method_name}_confidence'] = method_data['confidence']\n",
    "                else:\n",
    "                    # If it did NOT run, fill the columns with None (which becomes blank in the CSV).\n",
    "                    row[f'{method_name}_prediction'] = None\n",
    "                    row[f'{method_name}_confidence'] = None\n",
    "            \n",
    "            wide_format_results.append(row)\n",
    "\n",
    "        # Create the final, detailed DataFrame and save it.\n",
    "        results_df = pd.DataFrame(wide_format_results)\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n Adaptive processing complete. DETAILED WIDE-FORMAT results saved to: {output_file}\")\n",
    "        \n",
    "        #  END OF MODIFIED SECTION \n",
    "\n",
    "        # The final evaluation report can still be printed for convenience\n",
    "        print(\"\\n--- ADAPTIVE MODE OVERALL ENSEMBLE PERFORMANCE ---\")\n",
    "        # We need the full results dict to run the original evaluation function\n",
    "        full_results_df_for_eval = pd.DataFrame(adaptive_results)\n",
    "        performance = helios.evaluate_performance(full_results_df_for_eval)\n",
    "        if 'ensemble_performance' in performance:\n",
    "            ens_perf = performance['ensemble_performance']\n",
    "            print(f\"  - F1 Score:  {ens_perf['f1']:.3f}\")\n",
    "            print(f\"  - Precision: {ens_perf['precision']:.3f}\")\n",
    "            print(f\"  - Recall:    {ens_perf['recall']:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error during adaptive run: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T15:32:35.307888Z",
     "iopub.status.busy": "2025-09-26T15:32:35.307636Z",
     "iopub.status.idle": "2025-09-26T15:52:19.964769Z",
     "shell.execute_reply": "2025-09-26T15:52:19.964121Z",
     "shell.execute_reply.started": "2025-09-26T15:32:35.307871Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        HELIOS v2.0 EXECUTION               \n",
      "   Unified Hallucination Framework          \n",
      "\n",
      "=== Setting up HELIOS v2.0 System ===\n",
      "Loading meta-llama/Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b82e31598404d28a274fd576fa45a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      " All models loaded successfully!\n",
      "\n",
      "============================================================\n",
      "PHASE 1: ADAPTIVE PROCESSING (SINGLE PATH PER QUERY)\n",
      "============================================================\n",
      "=== Running HELIOS in Adaptive Mode on /kaggle/input/small-dataset/very_small_test (1).csv ===\n",
      "Processing 10 examples adaptively...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adaptive Processing:   0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627e9443b6d94297b23ff956a1ee9e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d09e8a4b484ab2a0320a072a657b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  10%|         | 1/10 [00:20<03:05, 20.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.DETECTION_ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148bcc509bbe4e14adc51dcaf9089f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a576ca3697a043c9ac3e71f6c52cc98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  20%|        | 2/10 [04:00<18:22, 137.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c386a44e8364529956cc9c00e7b2213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33937f6890844d45b5ff16531c1d70ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  30%|       | 3/10 [05:58<15:02, 128.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1096f15411c343b9bbdf3991c54c070f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708e31fb723a4737815bdf8125da0ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  40%|      | 4/10 [07:54<12:23, 123.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244faa0b2709457b9de7d0f85352548d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d8fa6759c64f52b6ba917c2861b537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  50%|     | 5/10 [09:53<10:10, 122.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.DETECTION_ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57944be1c4f48e99224c8f2d771f871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1286dee9a010447d86d3f4e034c0d01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  60%|    | 6/10 [11:17<07:16, 109.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.DETECTION_ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999890d6c8b34d0585e0a0c424de0eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561bb3e093bc481090509a7cfd90c843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  70%|   | 7/10 [13:01<05:22, 107.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06943dbf89af44b48d06bc8e4f987d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf6cabf79ad47f0aabcb619b1b10a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  80%|  | 8/10 [14:47<03:33, 106.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.DETECTION_ONLY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc9a635e60e4216b16cd0aae985948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66fd89ecc604651a9fc48398d998e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing:  90%| | 9/10 [16:53<01:52, 112.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingMode.MITIGATION_GUIDED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c742f883975c4f428fb2e17a903c4c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ea5ffe651845b6a7f7947bcb63236c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Adaptive Processing: 100%|| 10/10 [18:45<00:00, 112.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Adaptive processing complete. DETAILED WIDE-FORMAT results saved to: ./helios_adaptive_results3.csv\n",
      "\n",
      "--- ADAPTIVE MODE OVERALL ENSEMBLE PERFORMANCE ---\n",
      "  - F1 Score:  0.667\n",
      "  - Precision: 0.600\n",
      "  - Recall:    0.750\n",
      "\n",
      "============================================================\n",
      "HELIOS v2.0 ADAPTIVE EXECUTION COMPLETE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function that demonstrates the complete HELIOS system\n",
    "    \"\"\"\n",
    "    print(\"\")\n",
    "    print(\"        HELIOS v2.0 EXECUTION               \") \n",
    "    print(\"   Unified Hallucination Framework          \")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Initialize HELIOS system\n",
    "    helios = setup_helios_system()\n",
    "    if not helios:\n",
    "        print(\"Failed to initialize HELIOS. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Configuration for dataset path\n",
    "    input_file = '/kaggle/input/small-dataset/very_small_test (1).csv'\n",
    "    output_dir = './'\n",
    "    \n",
    "    # Create dummy file if it doesn't exist for testing\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Input file not found. Creating a dummy file at '{input_file}'\")\n",
    "        dummy_data = {\n",
    "            'question': [\"Is it true that the Amazon River is the longest river in the world and that it flows through Brazil, Colombia, and Peru?\", \"Analyze why the Renaissance began in Italy and not in Spain, considering the economic impact of trade routes.?\", \"Tell me about the habitat of the red panda.\"],\n",
    "            'answer': [\"Yes, the Amazon River is widely considered the longest river, surpassing the Nile. It originates in the Andes Mountains in Peru and flows through Colombia and Brazil before emptying into the Atlantic Ocean.\", \"The Renaissance began in Italy due to a confluence of factors. Economically, Italian city-states like Venice and Florence dominated Mediterranean trade routes, creating immense wealth and a powerful merchant class that became patrons of the arts...\", \"The red panda lives in temperate forests in the eastern Himalayas and southwestern China.\"],\n",
    "            'hallucination_label': [False, True, False]\n",
    "        }\n",
    "        pd.DataFrame(dummy_data).to_csv(input_file, index=False)\n",
    "\n",
    "    # Define the output file for the adaptive results\n",
    "    adaptive_output_file = os.path.join(output_dir, 'helios_adaptive_results3.csv')\n",
    "    \n",
    "    # Run the adaptive processing function\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: ADAPTIVE PROCESSING (SINGLE PATH PER QUERY)\")\n",
    "    print(\"=\"*60)\n",
    "    run_helios_adaptive(helios, input_file, adaptive_output_file)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HELIOS v2.0 ADAPTIVE EXECUTION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Execute main function when script is run\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-26T16:01:42.504285Z",
     "iopub.status.busy": "2025-09-26T16:01:42.503970Z",
     "iopub.status.idle": "2025-09-26T16:01:42.865225Z",
     "shell.execute_reply": "2025-09-26T16:01:42.864602Z",
     "shell.execute_reply.started": "2025-09-26T16:01:42.504262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Visualization from WIDE Format CSV File ---\n",
      "Successfully loaded './helios_adaptive_results3.csv' for analysis.\n",
      "Data contains 10 rows (one per query).\n",
      "Analyzing 'KGR': Found 4 queries where it was run.\n",
      "Analyzing 'CoVE': Found 8 queries where it was run.\n",
      "Analyzing 'Stitch': Found 10 queries where it was run.\n",
      "Analyzing 'InterrogateLLM': Found 10 queries where it was run.\n",
      "\n",
      "Calculated Performance Metrics for Visualization:\n",
      "             Component  F1-Score  Precision    Recall\n",
      "0  Ensemble (Adaptive)  0.666667   0.600000  0.750000\n",
      "1                  KGR  1.000000   1.000000  1.000000\n",
      "2                 CoVE  0.666667   0.666667  0.666667\n",
      "3               Stitch  0.600000   0.500000  0.750000\n",
      "4       InterrogateLLM  0.615385   0.444444  1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CORRECTED CELL FOR VISUALIZING ADAPTIVE RUN RESULTS FROM A \"WIDE\" FORMAT CSV\n",
    "# This version correctly handles missing data to provide accurate metrics.\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"--- Starting Visualization from WIDE Format CSV File ---\")\n",
    "\n",
    "# --- 1. Load the wide-format results file ---\n",
    "file_path = './helios_adaptive_results3.csv'\n",
    "\n",
    "try:\n",
    "    df_wide = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded '{file_path}' for analysis.\")\n",
    "    print(f\"Data contains {df_wide.shape[0]} rows (one per query).\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{file_path}' was not found.\")\n",
    "    print(\"Please make sure the main script has run successfully.\")\n",
    "    df_wide = pd.DataFrame()\n",
    "\n",
    "if not df_wide.empty:\n",
    "    # Set a professional plot style suitable for papers\n",
    "    sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "\n",
    "    # --- 2. Calculate Metrics Accurately ---\n",
    "    performance_data = []\n",
    "    \n",
    "    # First, calculate overall Ensemble performance. This is straightforward.\n",
    "    y_true_full_dataset = df_wide['ground_truth'].astype(bool)\n",
    "    y_pred_ensemble = df_wide['ensemble_prediction'].astype(bool)\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Component': 'Ensemble (Adaptive)',\n",
    "        'F1-Score': f1_score(y_true_full_dataset, y_pred_ensemble, zero_division=0),\n",
    "        'Precision': precision_score(y_true_full_dataset, y_pred_ensemble, zero_division=0),\n",
    "        'Recall': recall_score(y_true_full_dataset, y_pred_ensemble, zero_division=0)\n",
    "    })\n",
    "    \n",
    "    # Second, calculate performance for each individual method CORRECTLY.\n",
    "    all_methods = ['KGR', 'CoVE', 'Stitch', 'InterrogateLLM']\n",
    "    for method_name in all_methods:\n",
    "        prediction_column = f'{method_name}_prediction'\n",
    "        \n",
    "        # THE CRITICAL STEP: Create a temporary DataFrame that contains ONLY the rows\n",
    "        # where this specific method was actually run (i.e., the prediction is not blank).\n",
    "        method_subset_df = df_wide.dropna(subset=[prediction_column])\n",
    "        \n",
    "        print(f\"Analyzing '{method_name}': Found {len(method_subset_df)} queries where it was run.\")\n",
    "        \n",
    "        if not method_subset_df.empty:\n",
    "            # Now, calculate metrics ONLY on this clean subset of data.\n",
    "            y_true_method = method_subset_df['ground_truth'].astype(bool)\n",
    "            y_pred_method = method_subset_df[prediction_column].astype(bool)\n",
    "            \n",
    "            performance_data.append({\n",
    "                'Component': method_name,\n",
    "                'F1-Score': f1_score(y_true_method, y_pred_method, zero_division=0),\n",
    "                'Precision': precision_score(y_true_method, y_pred_method, zero_division=0),\n",
    "                'Recall': recall_score(y_true_method, y_pred_method, zero_division=0)\n",
    "            })\n",
    "\n",
    "    # --- 3. Create the Plot (this part remains the same) ---\n",
    "    perf_df = pd.DataFrame(performance_data)\n",
    "    print(\"\\nCalculated Performance Metrics for Visualization:\")\n",
    "    print(perf_df.to_string())\n",
    "\n",
    "    # \"Melt\" the DataFrame to prepare it for Seaborn's grouped bar plot\n",
    "    df_melted = perf_df.melt(id_vars='Component', \n",
    "                             value_vars=['F1-Score', 'Precision', 'Recall'],\n",
    "                             var_name='Metric', \n",
    "                             value_name='Score')\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(data=df_melted, x='Component', y='Score', hue='Metric', palette='plasma')\n",
    "    \n",
    "    plt.title('Adaptive Run: Ensemble vs. Individual Method Performance', fontsize=16, pad=20)\n",
    "    plt.xlabel('Detection Component', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), \n",
    "                    textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8252616,
     "sourceId": 13033311,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8269904,
     "sourceId": 13059347,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8270032,
     "sourceId": 13059565,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8352089,
     "sourceId": 13179789,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8352140,
     "sourceId": 13179862,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
